{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#use case using single layer perceptron that consists of only one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets('MNIST_data',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating nodes for input images and target output using placeholder\n",
    "x=tf.placeholder(tf.float32,shape=[None,784])\n",
    "y=tf.placeholder(tf.float32,shape=[None,10])\n",
    "\n",
    "w=tf.Variable(tf.zeros([784,10]))\n",
    "b=tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialising variables w and b\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3276\n",
      "0.3166\n",
      "0.5555\n",
      "0.5376\n",
      "0.6485\n",
      "0.7428\n",
      "0.769\n",
      "0.8024\n",
      "0.7429\n",
      "0.8122\n",
      "0.8012\n",
      "0.7888\n",
      "0.7986\n",
      "0.7128\n",
      "0.7763\n",
      "0.8152\n",
      "0.8256\n",
      "0.8474\n",
      "0.8103\n",
      "0.8312\n",
      "0.8399\n",
      "0.8419\n",
      "0.8404\n",
      "0.8447\n",
      "0.8597\n",
      "0.8474\n",
      "0.822\n",
      "0.8433\n",
      "0.8316\n",
      "0.8694\n",
      "0.8708\n",
      "0.871\n",
      "0.862\n",
      "0.8554\n",
      "0.8412\n",
      "0.8561\n",
      "0.8546\n",
      "0.8437\n",
      "0.8601\n",
      "0.8699\n",
      "0.8804\n",
      "0.8675\n",
      "0.876\n",
      "0.8804\n",
      "0.8766\n",
      "0.8843\n",
      "0.8645\n",
      "0.8821\n",
      "0.8781\n",
      "0.8782\n",
      "0.8765\n",
      "0.8767\n",
      "0.8763\n",
      "0.8696\n",
      "0.8788\n",
      "0.8805\n",
      "0.8838\n",
      "0.8802\n",
      "0.8813\n",
      "0.879\n",
      "0.8837\n",
      "0.8743\n",
      "0.8825\n",
      "0.881\n",
      "0.8809\n",
      "0.889\n",
      "0.8775\n",
      "0.89\n",
      "0.8815\n",
      "0.8869\n",
      "0.8789\n",
      "0.889\n",
      "0.8855\n",
      "0.8808\n",
      "0.8807\n",
      "0.8823\n",
      "0.8895\n",
      "0.8896\n",
      "0.8906\n",
      "0.8869\n",
      "0.8883\n",
      "0.8874\n",
      "0.8885\n",
      "0.8927\n",
      "0.894\n",
      "0.8918\n",
      "0.8936\n",
      "0.894\n",
      "0.8862\n",
      "0.8831\n",
      "0.8859\n",
      "0.8922\n",
      "0.89\n",
      "0.8911\n",
      "0.8933\n",
      "0.8863\n",
      "0.8909\n",
      "0.8943\n",
      "0.8883\n",
      "0.8924\n",
      "0.8871\n",
      "0.8971\n",
      "0.8979\n",
      "0.8963\n",
      "0.8967\n",
      "0.8975\n",
      "0.8914\n",
      "0.8942\n",
      "0.8954\n",
      "0.892\n",
      "0.8954\n",
      "0.8979\n",
      "0.8951\n",
      "0.8919\n",
      "0.8953\n",
      "0.8896\n",
      "0.8878\n",
      "0.8918\n",
      "0.8959\n",
      "0.8967\n",
      "0.8903\n",
      "0.896\n",
      "0.8984\n",
      "0.897\n",
      "0.8978\n",
      "0.895\n",
      "0.9009\n",
      "0.8969\n",
      "0.884\n",
      "0.8888\n",
      "0.8963\n",
      "0.8976\n",
      "0.9002\n",
      "0.8983\n",
      "0.8999\n",
      "0.8959\n",
      "0.9001\n",
      "0.8915\n",
      "0.893\n",
      "0.8952\n",
      "0.8888\n",
      "0.8911\n",
      "0.8935\n",
      "0.8956\n",
      "0.9006\n",
      "0.9017\n",
      "0.9005\n",
      "0.8988\n",
      "0.9009\n",
      "0.8988\n",
      "0.8995\n",
      "0.8944\n",
      "0.8982\n",
      "0.9027\n",
      "0.9016\n",
      "0.9036\n",
      "0.8998\n",
      "0.9003\n",
      "0.9035\n",
      "0.9006\n",
      "0.9023\n",
      "0.8985\n",
      "0.9009\n",
      "0.9039\n",
      "0.9006\n",
      "0.9035\n",
      "0.8778\n",
      "0.8939\n",
      "0.9017\n",
      "0.9019\n",
      "0.9033\n",
      "0.9029\n",
      "0.9034\n",
      "0.9007\n",
      "0.9005\n",
      "0.8987\n",
      "0.9027\n",
      "0.8994\n",
      "0.898\n",
      "0.9036\n",
      "0.905\n",
      "0.9045\n",
      "0.9025\n",
      "0.9052\n",
      "0.9029\n",
      "0.9022\n",
      "0.9028\n",
      "0.9025\n",
      "0.9006\n",
      "0.9038\n",
      "0.9035\n",
      "0.9061\n",
      "0.9052\n",
      "0.9055\n",
      "0.9067\n",
      "0.9058\n",
      "0.9026\n",
      "0.904\n",
      "0.8947\n",
      "0.9041\n",
      "0.9059\n",
      "0.8991\n",
      "0.8978\n",
      "0.9036\n",
      "0.9021\n",
      "0.9052\n",
      "0.9012\n",
      "0.9031\n",
      "0.9\n",
      "0.9041\n",
      "0.9064\n",
      "0.9016\n",
      "0.9034\n",
      "0.9075\n",
      "0.9042\n",
      "0.9059\n",
      "0.8987\n",
      "0.9079\n",
      "0.9013\n",
      "0.9056\n",
      "0.901\n",
      "0.8993\n",
      "0.8995\n",
      "0.9036\n",
      "0.9047\n",
      "0.9078\n",
      "0.9066\n",
      "0.8964\n",
      "0.9068\n",
      "0.9069\n",
      "0.9065\n",
      "0.909\n",
      "0.8996\n",
      "0.9068\n",
      "0.9027\n",
      "0.9064\n",
      "0.9082\n",
      "0.9079\n",
      "0.9073\n",
      "0.906\n",
      "0.9015\n",
      "0.9053\n",
      "0.9051\n",
      "0.9069\n",
      "0.9056\n",
      "0.9078\n",
      "0.9076\n",
      "0.9088\n",
      "0.908\n",
      "0.9078\n",
      "0.8979\n",
      "0.9021\n",
      "0.9078\n",
      "0.9068\n",
      "0.9075\n",
      "0.9069\n",
      "0.9072\n",
      "0.9051\n",
      "0.9072\n",
      "0.9064\n",
      "0.9036\n",
      "0.9046\n",
      "0.9066\n",
      "0.9078\n",
      "0.9055\n",
      "0.908\n",
      "0.9079\n",
      "0.9024\n",
      "0.9064\n",
      "0.9083\n",
      "0.9081\n",
      "0.907\n",
      "0.9094\n",
      "0.9067\n",
      "0.9078\n",
      "0.9076\n",
      "0.9093\n",
      "0.9095\n",
      "0.9054\n",
      "0.9078\n",
      "0.9086\n",
      "0.8985\n",
      "0.9038\n",
      "0.9055\n",
      "0.9042\n",
      "0.9077\n",
      "0.8981\n",
      "0.9068\n",
      "0.9057\n",
      "0.9085\n",
      "0.9097\n",
      "0.9091\n",
      "0.8988\n",
      "0.9065\n",
      "0.9082\n",
      "0.9073\n",
      "0.906\n",
      "0.9071\n",
      "0.9091\n",
      "0.9062\n",
      "0.9076\n",
      "0.9072\n",
      "0.9095\n",
      "0.9077\n",
      "0.9084\n",
      "0.9074\n",
      "0.9085\n",
      "0.9102\n",
      "0.9041\n",
      "0.9078\n",
      "0.9034\n",
      "0.9064\n",
      "0.905\n",
      "0.9111\n",
      "0.9106\n",
      "0.9108\n",
      "0.9077\n",
      "0.908\n",
      "0.909\n",
      "0.9105\n",
      "0.9065\n",
      "0.911\n",
      "0.911\n",
      "0.9097\n",
      "0.9112\n",
      "0.9084\n",
      "0.9105\n",
      "0.9069\n",
      "0.9126\n",
      "0.9099\n",
      "0.9102\n",
      "0.9129\n",
      "0.9084\n",
      "0.9131\n",
      "0.9085\n",
      "0.9082\n",
      "0.9101\n",
      "0.912\n",
      "0.9119\n",
      "0.904\n",
      "0.9114\n",
      "0.9045\n",
      "0.9039\n",
      "0.9116\n",
      "0.9113\n",
      "0.9114\n",
      "0.9127\n",
      "0.9112\n",
      "0.9111\n",
      "0.9068\n",
      "0.9066\n",
      "0.9063\n",
      "0.9096\n",
      "0.9096\n",
      "0.9103\n",
      "0.9089\n",
      "0.9123\n",
      "0.9108\n",
      "0.9113\n",
      "0.9099\n",
      "0.9104\n",
      "0.9092\n",
      "0.9054\n",
      "0.9084\n",
      "0.9032\n",
      "0.9113\n",
      "0.909\n",
      "0.9016\n",
      "0.9097\n",
      "0.9094\n",
      "0.911\n",
      "0.9058\n",
      "0.9119\n",
      "0.9104\n",
      "0.9109\n",
      "0.9129\n",
      "0.9066\n",
      "0.9109\n",
      "0.9089\n",
      "0.908\n",
      "0.9126\n",
      "0.9104\n",
      "0.9122\n",
      "0.9117\n",
      "0.9091\n",
      "0.9095\n",
      "0.9026\n",
      "0.9112\n",
      "0.9073\n",
      "0.9067\n",
      "0.9097\n",
      "0.9093\n",
      "0.9102\n",
      "0.9088\n",
      "0.9077\n",
      "0.9096\n",
      "0.9119\n",
      "0.9127\n",
      "0.9082\n",
      "0.9086\n",
      "0.9068\n",
      "0.9079\n",
      "0.909\n",
      "0.9124\n",
      "0.9076\n",
      "0.9103\n",
      "0.9118\n",
      "0.9126\n",
      "0.9056\n",
      "0.9059\n",
      "0.9074\n",
      "0.9136\n",
      "0.9067\n",
      "0.9133\n",
      "0.9082\n",
      "0.9097\n",
      "0.9043\n",
      "0.9031\n",
      "0.9096\n",
      "0.9058\n",
      "0.9107\n",
      "0.9048\n",
      "0.9131\n",
      "0.9117\n",
      "0.9114\n",
      "0.9143\n",
      "0.9134\n",
      "0.9121\n",
      "0.9143\n",
      "0.915\n",
      "0.9128\n",
      "0.9141\n",
      "0.9137\n",
      "0.9095\n",
      "0.9118\n",
      "0.9142\n",
      "0.9104\n",
      "0.9126\n",
      "0.9137\n",
      "0.9132\n",
      "0.9105\n",
      "0.9146\n",
      "0.9114\n",
      "0.9112\n",
      "0.9078\n",
      "0.9129\n",
      "0.912\n",
      "0.9127\n",
      "0.9113\n",
      "0.9123\n",
      "0.9113\n",
      "0.9108\n",
      "0.911\n",
      "0.9103\n",
      "0.9125\n",
      "0.9139\n",
      "0.9143\n",
      "0.9148\n",
      "0.9126\n",
      "0.9125\n",
      "0.9131\n",
      "0.9124\n",
      "0.9107\n",
      "0.9123\n",
      "0.9142\n",
      "0.9116\n",
      "0.9133\n",
      "0.9142\n",
      "0.9156\n",
      "0.9131\n",
      "0.9102\n",
      "0.9093\n",
      "0.9117\n",
      "0.913\n",
      "0.9157\n",
      "0.9128\n",
      "0.9152\n",
      "0.9156\n",
      "0.9146\n",
      "0.9142\n",
      "0.9124\n",
      "0.9134\n",
      "0.9133\n",
      "0.9147\n",
      "0.9151\n",
      "0.9141\n",
      "0.9123\n",
      "0.9098\n",
      "0.9104\n",
      "0.9135\n",
      "0.9131\n",
      "0.9127\n",
      "0.9127\n",
      "0.9147\n",
      "0.9108\n",
      "0.9131\n",
      "0.9068\n",
      "0.9129\n",
      "0.9122\n",
      "0.9144\n",
      "0.9147\n",
      "0.9102\n",
      "0.9108\n",
      "0.9118\n",
      "0.9124\n",
      "0.9109\n",
      "0.9154\n",
      "0.9134\n",
      "0.9117\n",
      "0.908\n",
      "0.9152\n",
      "0.9165\n",
      "0.9122\n",
      "0.9086\n",
      "0.9129\n",
      "0.9158\n",
      "0.9144\n",
      "0.9125\n",
      "0.9104\n",
      "0.9175\n",
      "0.9138\n",
      "0.9143\n",
      "0.9146\n",
      "0.9142\n",
      "0.9136\n",
      "0.9141\n",
      "0.9152\n",
      "0.9135\n",
      "0.9157\n",
      "0.9136\n",
      "0.9145\n",
      "0.9146\n",
      "0.9147\n",
      "0.9163\n",
      "0.9164\n",
      "0.9156\n",
      "0.9153\n",
      "0.9163\n",
      "0.9163\n",
      "0.9152\n",
      "0.9172\n",
      "0.916\n",
      "0.9143\n",
      "0.9119\n",
      "0.9178\n",
      "0.9163\n",
      "0.9073\n",
      "0.9065\n",
      "0.9142\n",
      "0.9126\n",
      "0.916\n",
      "0.909\n",
      "0.9089\n",
      "0.9077\n",
      "0.9162\n",
      "0.9157\n",
      "0.9109\n",
      "0.9129\n",
      "0.915\n",
      "0.9145\n",
      "0.9098\n",
      "0.9059\n",
      "0.9112\n",
      "0.9123\n",
      "0.9145\n",
      "0.915\n",
      "0.9158\n",
      "0.915\n",
      "0.9107\n",
      "0.911\n",
      "0.9102\n",
      "0.9137\n",
      "0.9154\n",
      "0.9146\n",
      "0.9125\n",
      "0.9133\n",
      "0.9119\n",
      "0.91\n",
      "0.9131\n",
      "0.9119\n",
      "0.913\n",
      "0.9158\n",
      "0.9136\n",
      "0.916\n",
      "0.9164\n",
      "0.9141\n",
      "0.9106\n",
      "0.9142\n",
      "0.9123\n",
      "0.9122\n",
      "0.9049\n",
      "0.9117\n",
      "0.9142\n",
      "0.912\n",
      "0.9169\n",
      "0.9144\n",
      "0.9144\n",
      "0.9145\n",
      "0.9141\n",
      "0.9147\n",
      "0.9151\n",
      "0.9148\n",
      "0.9165\n",
      "0.9153\n",
      "0.9142\n",
      "0.9052\n",
      "0.9142\n",
      "0.9162\n",
      "0.9143\n",
      "0.9176\n",
      "0.9087\n",
      "0.9105\n",
      "0.9143\n",
      "0.9131\n",
      "0.9148\n",
      "0.9112\n",
      "0.9132\n",
      "0.9135\n",
      "0.9143\n",
      "0.9157\n",
      "0.9117\n",
      "0.9113\n",
      "0.9146\n",
      "0.9153\n",
      "0.9186\n",
      "0.9175\n",
      "0.913\n",
      "0.9135\n",
      "0.9168\n",
      "0.9163\n",
      "0.9155\n",
      "0.9155\n",
      "0.9151\n",
      "0.917\n",
      "0.9123\n",
      "0.9175\n",
      "0.9173\n",
      "0.9122\n",
      "0.9126\n",
      "0.9155\n",
      "0.9144\n",
      "0.9152\n",
      "0.9122\n",
      "0.9157\n",
      "0.9153\n",
      "0.9184\n",
      "0.9173\n",
      "0.9178\n",
      "0.914\n",
      "0.9175\n",
      "0.9167\n",
      "0.9166\n",
      "0.9153\n",
      "0.9172\n",
      "0.9156\n",
      "0.9175\n",
      "0.9156\n",
      "0.9144\n",
      "0.9166\n",
      "0.9142\n",
      "0.9142\n",
      "0.9157\n",
      "0.9164\n",
      "0.9159\n",
      "0.9168\n",
      "0.918\n",
      "0.9176\n",
      "0.9145\n",
      "0.9166\n",
      "0.9166\n",
      "0.9158\n",
      "0.9148\n",
      "0.9142\n",
      "0.9159\n",
      "0.913\n",
      "0.9148\n",
      "0.9147\n",
      "0.9194\n",
      "0.9179\n",
      "0.917\n",
      "0.9156\n",
      "0.9177\n",
      "0.9187\n",
      "0.9181\n",
      "0.9157\n",
      "0.9094\n",
      "0.9121\n",
      "0.9157\n",
      "0.9163\n",
      "0.9164\n",
      "0.9186\n",
      "0.9135\n",
      "0.9162\n",
      "0.9135\n",
      "0.9162\n",
      "0.9114\n",
      "0.9095\n",
      "0.9137\n",
      "0.915\n",
      "0.9158\n",
      "0.915\n",
      "0.9101\n",
      "0.9087\n",
      "0.9149\n",
      "0.9127\n",
      "0.9164\n",
      "0.916\n",
      "0.9164\n",
      "0.9182\n",
      "0.9152\n",
      "0.916\n",
      "0.9167\n",
      "0.9154\n",
      "0.9155\n",
      "0.9172\n",
      "0.9151\n",
      "0.9183\n",
      "0.9176\n",
      "0.9172\n",
      "0.9163\n",
      "0.9144\n",
      "0.9147\n",
      "0.9131\n",
      "0.9154\n",
      "0.9164\n",
      "0.9129\n",
      "0.9151\n",
      "0.913\n",
      "0.9146\n",
      "0.9127\n",
      "0.9145\n",
      "0.9132\n",
      "0.9105\n",
      "0.9096\n",
      "0.9159\n",
      "0.9088\n",
      "0.9168\n",
      "0.9175\n",
      "0.9173\n",
      "0.9174\n",
      "0.9171\n",
      "0.916\n",
      "0.9161\n",
      "0.9174\n",
      "0.9171\n",
      "0.9151\n",
      "0.9147\n",
      "0.9189\n",
      "0.9187\n",
      "0.9133\n",
      "0.9169\n",
      "0.9165\n",
      "0.9125\n",
      "0.9164\n",
      "0.9181\n",
      "0.9171\n",
      "0.9185\n",
      "0.918\n",
      "0.9164\n",
      "0.9175\n",
      "0.9171\n",
      "0.9145\n",
      "0.9175\n",
      "0.9165\n",
      "0.9162\n",
      "0.9151\n",
      "0.9095\n",
      "0.9162\n",
      "0.9129\n",
      "0.9169\n",
      "0.9161\n",
      "0.9128\n",
      "0.9154\n",
      "0.9173\n",
      "0.9184\n",
      "0.9184\n",
      "0.9193\n",
      "0.9191\n",
      "0.9164\n",
      "0.9169\n",
      "0.9194\n",
      "0.9179\n",
      "0.9206\n",
      "0.9161\n",
      "0.9187\n",
      "0.9182\n",
      "0.918\n",
      "0.9172\n",
      "0.9159\n",
      "0.9172\n",
      "0.9174\n",
      "0.9157\n",
      "0.9128\n",
      "0.9178\n",
      "0.9183\n",
      "0.9173\n",
      "0.9162\n",
      "0.9153\n",
      "0.9136\n",
      "0.9154\n",
      "0.9183\n",
      "0.9165\n",
      "0.92\n",
      "0.9201\n",
      "0.9172\n",
      "0.9186\n",
      "0.9176\n",
      "0.9182\n",
      "0.9165\n",
      "0.9157\n",
      "0.9166\n",
      "0.9124\n",
      "0.9154\n",
      "0.9187\n",
      "0.9188\n",
      "0.917\n",
      "0.9171\n",
      "0.9159\n",
      "0.9175\n",
      "0.9163\n",
      "0.9135\n",
      "0.9192\n",
      "0.9198\n",
      "0.9193\n",
      "0.9184\n",
      "0.9166\n",
      "0.9178\n",
      "0.9177\n",
      "0.9181\n",
      "0.9198\n",
      "0.9189\n",
      "0.9201\n",
      "0.9195\n",
      "0.9198\n",
      "0.9193\n",
      "0.9196\n",
      "0.9185\n",
      "0.9208\n",
      "0.9205\n",
      "0.919\n",
      "0.9204\n",
      "0.9208\n",
      "0.919\n",
      "0.9175\n",
      "0.9177\n",
      "0.9132\n",
      "0.9141\n",
      "0.9146\n",
      "0.9176\n",
      "0.9171\n",
      "0.9176\n",
      "0.9202\n",
      "0.9129\n",
      "0.9148\n",
      "0.918\n",
      "0.9197\n",
      "0.922\n",
      "0.9212\n",
      "0.9204\n",
      "0.9203\n",
      "0.916\n",
      "0.9179\n",
      "0.9144\n",
      "0.9127\n",
      "0.9147\n",
      "0.9187\n",
      "0.9178\n",
      "0.9157\n",
      "0.9143\n",
      "0.9127\n",
      "0.9146\n",
      "0.9202\n",
      "0.9181\n",
      "0.9185\n",
      "0.9168\n",
      "0.9159\n",
      "0.9219\n",
      "0.9199\n",
      "0.9188\n",
      "0.9198\n",
      "0.9166\n",
      "0.918\n",
      "0.9172\n",
      "0.9157\n",
      "0.9195\n",
      "0.9124\n",
      "0.9149\n",
      "0.9173\n",
      "0.918\n",
      "0.92\n",
      "0.9183\n",
      "0.9145\n",
      "0.9204\n",
      "0.9191\n",
      "0.9174\n",
      "0.9178\n",
      "0.9204\n",
      "0.9191\n",
      "0.9158\n",
      "0.9175\n",
      "0.9158\n",
      "0.9157\n",
      "0.9184\n",
      "0.9183\n",
      "0.9178\n",
      "0.918\n",
      "0.915\n",
      "0.9183\n",
      "0.9131\n",
      "0.918\n",
      "0.9202\n",
      "0.915\n",
      "0.9174\n",
      "0.9159\n",
      "0.9143\n",
      "0.9175\n",
      "0.9167\n",
      "0.9166\n",
      "0.9196\n",
      "0.9187\n",
      "0.919\n",
      "0.9163\n",
      "0.9186\n",
      "0.9182\n",
      "0.9184\n",
      "0.9144\n",
      "0.9181\n",
      "0.9196\n",
      "0.9181\n",
      "0.9203\n",
      "0.9145\n",
      "0.9146\n",
      "0.9169\n",
      "0.9195\n",
      "0.9202\n",
      "0.9197\n",
      "0.9168\n",
      "0.9195\n",
      "0.9163\n",
      "0.918\n",
      "0.9185\n",
      "0.9197\n",
      "0.9178\n",
      "0.9188\n",
      "0.918\n",
      "0.9179\n",
      "0.9149\n",
      "0.9163\n",
      "0.9174\n",
      "0.9183\n",
      "0.9193\n",
      "0.918\n",
      "0.9164\n",
      "0.9176\n",
      "0.9183\n",
      "0.912\n",
      "0.9204\n",
      "0.921\n",
      "0.9191\n",
      "0.9164\n",
      "0.917\n",
      "0.9182\n",
      "0.9175\n",
      "0.9162\n",
      "0.9177\n",
      "0.9161\n",
      "0.9182\n",
      "0.9168\n",
      "0.9156\n",
      "0.9151\n",
      "0.918\n",
      "0.9188\n",
      "0.9189\n",
      "0.9182\n",
      "0.9179\n",
      "0.9169\n",
      "0.9175\n",
      "0.9148\n",
      "0.9149\n",
      "0.915\n",
      "0.9166\n",
      "0.9158\n",
      "0.9167\n",
      "0.9145\n",
      "0.914\n",
      "0.9156\n",
      "0.9156\n",
      "0.9124\n",
      "0.919\n",
      "0.9184\n",
      "0.9178\n",
      "0.9185\n",
      "0.9102\n",
      "0.9145\n",
      "0.9156\n",
      "0.9179\n",
      "0.9168\n",
      "0.9175\n",
      "0.9188\n",
      "0.9152\n"
     ]
    }
   ],
   "source": [
    "#model\n",
    "y_=tf.matmul(x,w)+b\n",
    "#cross entroopy contains mean of difference between predicted and actual value \n",
    "cross_entropy=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=y_))\n",
    "#training model and minimizing cross entropy\n",
    "train_step=tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "#loading 100 training examples in each training iteration\n",
    "for _ in range(1000):\n",
    "    batch=mnist.train.next_batch(100)\n",
    "    train_step.run(feed_dict={x:batch[0],y:batch[1]})\n",
    "    #to see if predicted and actual values are equal or not\n",
    "    correct_prediction=tf.equal(tf.argmax(y_,1),tf.argmax(y,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    print(accuracy.eval(feed_dict={x:mnist.test.images,y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same use case using multilayer perceptron that will consist of more than one hidden layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using tensorflow for sonar dataset\n",
    "import os\n",
    "os.chdir(\"F:\\\\sonar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading dataset\n",
    "def read_dataset():\n",
    "    df=pd.read_csv(\"sonar_csv.csv\")\n",
    "    X=df[df.columns[0:60]].values\n",
    "    y=df[df.columns[60]]\n",
    "    \n",
    "    encoder=LabelEncoder()\n",
    "    encoder.fit(y)\n",
    "    y=encoder.transform(y)\n",
    "    Y=one_hot_encode(y)\n",
    "    print(X.shape)\n",
    "    return(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining encoder function\n",
    "def one_hot_encode(labels):\n",
    "    n_labels=len(labels)\n",
    "    n_unique_labels=len(np.unique(labels))\n",
    "    one_hot_encode=np.zeros((n_labels,n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels),labels]=1\n",
    "    return one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 60)\n"
     ]
    }
   ],
   "source": [
    "X,Y=read_dataset()\n",
    "#shuffling dataset to mix up rows\n",
    "X,Y=shuffle(X,Y,random_state=1)\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(X,Y,test_size=0.20,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166, 2)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining important parameters and variables to work with tensor\n",
    "learning_rate=0.3\n",
    "training_epochs=1000\n",
    "cost_history=np.empty(shape=[1],dtype=float)\n",
    "n_dim=X.shape[1]\n",
    "n_class=2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"F://tensorflow multilayer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining no of hidden layers\n",
    "n_hidden_1=60\n",
    "n_hidden_2=60\n",
    "n_hidden_3=60\n",
    "n_hidden_4=60\n",
    "\n",
    "x=tf.placeholder(tf.float32,[None,n_dim])\n",
    "w=tf.Variable(tf.zeros([n_dim,n_class]))\n",
    "w=tf.Variable(tf.zeros([n_class]))\n",
    "y_=tf.placeholder(tf.float32,[None,n_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166, 2)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining model\n",
    "def multi_layer_perceptron(x,weights,biases):\n",
    "    layer1=tf.add(tf.matmul(x,weights['h1']),biases['b1'])\n",
    "    layer1=tf.nn.sigmoid(layer1)\n",
    "    \n",
    "    layer2=tf.add(tf.matmul(layer1,weights['h2']),biases['b2'])\n",
    "    layer2=tf.nn.sigmoid(layer2)\n",
    "    \n",
    "    layer3=tf.add(tf.matmul(layer2,weights['h3']),biases['b3'])\n",
    "    layer3=tf.nn.sigmoid(layer3)\n",
    "    \n",
    "    layer4=tf.add(tf.matmul(layer3,weights['h4']),biases['b4'])\n",
    "    layer4=tf.nn.sigmoid(layer4)\n",
    "    \n",
    "    output_layer=tf.matmul(layer4,weights['out'])+ biases['out']\n",
    "    return output_layer\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights={'h1':tf.Variable(tf.truncated_normal([n_dim,n_hidden_1])),\n",
    "         'h2':tf.Variable(tf.truncated_normal([n_hidden_1,n_hidden_2])),\n",
    "         'h3':tf.Variable(tf.truncated_normal([n_hidden_2,n_hidden_3])),\n",
    "         'h4':tf.Variable(tf.truncated_normal([n_hidden_3,n_hidden_4])),\n",
    "         'out':tf.Variable(tf.truncated_normal([n_hidden_4,n_class]))}\n",
    "\n",
    "biases={'b1':tf.Variable(tf.truncated_normal([n_hidden_1])),\n",
    "        'b2':tf.Variable(tf.truncated_normal([n_hidden_2])),\n",
    "        'b3':tf.Variable(tf.truncated_normal([n_hidden_3])),\n",
    "        'b4':tf.Variable(tf.truncated_normal([n_hidden_4])),\n",
    "        'out':tf.Variable(tf.truncated_normal([n_class]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing all variables\n",
    "init=tf.global_variables_initializer()\n",
    "saver=tf.train.Saver()\n",
    "\n",
    "y=multi_layer_perceptron(x,weights,biases)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y,labels=y_))\n",
    "training_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "#for predicting on given data to see output after training all below steps\n",
    "saver.restore(sess,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=tf.argmax(y,1)\n",
    "correct_prediction=tf.equal(prediction,argmax(y_,1))\n",
    "accuracy=tf.reduce_men(tf.cast(correct_prediction,tf.float32))\n",
    "#printing accuracy\n",
    "for i in range(93,101):\n",
    "    prediction_run=sess.run(prediction,feed-dict={x:x[i].reshape(1,60)})\n",
    "    accuracy_run=sess.run(accuracy,feed_dict={x:x[i].reshape(1,60)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - cost: 0.577634 -MSE: 1.1337826967143403 train_accuray: 0.6506024\n",
      "epoch: 1 - cost: 0.660079 -MSE: 1.2861457248250998 train_accuray: 0.6987952\n",
      "epoch: 2 - cost: 0.5749772 -MSE: 1.152603199764425 train_accuray: 0.6506024\n",
      "epoch: 3 - cost: 0.6577705 -MSE: 1.2922677811345091 train_accuray: 0.6987952\n",
      "epoch: 4 - cost: 0.5723358 -MSE: 1.171481309375259 train_accuray: 0.6566265\n",
      "epoch: 5 - cost: 0.6554747 -MSE: 1.2986251968028009 train_accuray: 0.6987952\n",
      "epoch: 6 - cost: 0.56970865 -MSE: 1.1903934141253738 train_accuray: 0.6566265\n",
      "epoch: 7 - cost: 0.6531896 -MSE: 1.3051837274310552 train_accuray: 0.6987952\n",
      "epoch: 8 - cost: 0.567095 -MSE: 1.2093176903568104 train_accuray: 0.6566265\n",
      "epoch: 9 - cost: 0.65091336 -MSE: 1.3119112475725014 train_accuray: 0.6987952\n",
      "epoch: 10 - cost: 0.5644944 -MSE: 1.2282355903342703 train_accuray: 0.6566265\n",
      "epoch: 11 - cost: 0.6486443 -MSE: 1.3187809667620853 train_accuray: 0.6987952\n",
      "epoch: 12 - cost: 0.56190604 -MSE: 1.2471313539304805 train_accuray: 0.6566265\n",
      "epoch: 13 - cost: 0.6463817 -MSE: 1.3257687053490563 train_accuray: 0.6987952\n",
      "epoch: 14 - cost: 0.55932975 -MSE: 1.2659900955256262 train_accuray: 0.6566265\n",
      "epoch: 15 - cost: 0.644124 -MSE: 1.3328507577714974 train_accuray: 0.6987952\n",
      "epoch: 16 - cost: 0.5567654 -MSE: 1.2847983111086738 train_accuray: 0.6566265\n",
      "epoch: 17 - cost: 0.64187086 -MSE: 1.3400100419689278 train_accuray: 0.70481926\n",
      "epoch: 18 - cost: 0.5542125 -MSE: 1.3035451944101781 train_accuray: 0.6566265\n",
      "epoch: 19 - cost: 0.63962084 -MSE: 1.347227543952427 train_accuray: 0.7108434\n",
      "epoch: 20 - cost: 0.55167085 -MSE: 1.3222202157870528 train_accuray: 0.6686747\n",
      "epoch: 21 - cost: 0.6373735 -MSE: 1.354487657793178 train_accuray: 0.7108434\n",
      "epoch: 22 - cost: 0.5491406 -MSE: 1.340815576931204 train_accuray: 0.6686747\n",
      "epoch: 23 - cost: 0.6351283 -MSE: 1.3617784496533811 train_accuray: 0.7108434\n",
      "epoch: 24 - cost: 0.5466213 -MSE: 1.3593243163737692 train_accuray: 0.6686747\n",
      "epoch: 25 - cost: 0.63288426 -MSE: 1.3690879143869543 train_accuray: 0.7108434\n",
      "epoch: 26 - cost: 0.54411304 -MSE: 1.377739742952046 train_accuray: 0.6686747\n",
      "epoch: 27 - cost: 0.6306415 -MSE: 1.376407286371225 train_accuray: 0.7108434\n",
      "epoch: 28 - cost: 0.54161644 -MSE: 1.396059558099944 train_accuray: 0.6686747\n",
      "epoch: 29 - cost: 0.6284 -MSE: 1.3837304425505772 train_accuray: 0.7108434\n",
      "epoch: 30 - cost: 0.5391312 -MSE: 1.4142794833552366 train_accuray: 0.6686747\n",
      "epoch: 31 - cost: 0.6261589 -MSE: 1.3910495131611393 train_accuray: 0.7108434\n",
      "epoch: 32 - cost: 0.5366573 -MSE: 1.4323957679227555 train_accuray: 0.6686747\n",
      "epoch: 33 - cost: 0.6239178 -MSE: 1.3983598786103715 train_accuray: 0.7108434\n",
      "epoch: 34 - cost: 0.53419507 -MSE: 1.4504087791947173 train_accuray: 0.6686747\n",
      "epoch: 35 - cost: 0.6216773 -MSE: 1.4056593478338817 train_accuray: 0.7108434\n",
      "epoch: 36 - cost: 0.53174484 -MSE: 1.4683188151673745 train_accuray: 0.6686747\n",
      "epoch: 37 - cost: 0.61943674 -MSE: 1.4129461277092428 train_accuray: 0.7108434\n",
      "epoch: 38 - cost: 0.5293066 -MSE: 1.486125658884887 train_accuray: 0.6686747\n",
      "epoch: 39 - cost: 0.6171967 -MSE: 1.4202200888152388 train_accuray: 0.7108434\n",
      "epoch: 40 - cost: 0.5268811 -MSE: 1.5038320812408046 train_accuray: 0.6686747\n",
      "epoch: 41 - cost: 0.61495715 -MSE: 1.4274819884408974 train_accuray: 0.7108434\n",
      "epoch: 42 - cost: 0.5244684 -MSE: 1.5214396916516482 train_accuray: 0.6686747\n",
      "epoch: 43 - cost: 0.61271816 -MSE: 1.434733195127761 train_accuray: 0.7108434\n",
      "epoch: 44 - cost: 0.5220687 -MSE: 1.5389517405570892 train_accuray: 0.6686747\n",
      "epoch: 45 - cost: 0.61047953 -MSE: 1.4419754433694114 train_accuray: 0.7108434\n",
      "epoch: 46 - cost: 0.5196821 -MSE: 1.556371879136385 train_accuray: 0.6686747\n",
      "epoch: 47 - cost: 0.6082414 -MSE: 1.4492114682747448 train_accuray: 0.7108434\n",
      "epoch: 48 - cost: 0.51730925 -MSE: 1.5737062836832596 train_accuray: 0.67469877\n",
      "epoch: 49 - cost: 0.60600466 -MSE: 1.4564472414417657 train_accuray: 0.7108434\n",
      "epoch: 50 - cost: 0.5149505 -MSE: 1.5909595064948683 train_accuray: 0.67469877\n",
      "epoch: 51 - cost: 0.6037692 -MSE: 1.4636860833305303 train_accuray: 0.7108434\n",
      "epoch: 52 - cost: 0.51260597 -MSE: 1.608136436841556 train_accuray: 0.67469877\n",
      "epoch: 53 - cost: 0.6015353 -MSE: 1.4709322055053646 train_accuray: 0.7108434\n",
      "epoch: 54 - cost: 0.5102757 -MSE: 1.625242842004302 train_accuray: 0.67469877\n",
      "epoch: 55 - cost: 0.5993029 -MSE: 1.4781902102996283 train_accuray: 0.7108434\n",
      "epoch: 56 - cost: 0.50796014 -MSE: 1.6422836791511775 train_accuray: 0.6807229\n",
      "epoch: 57 - cost: 0.597072 -MSE: 1.485463861612128 train_accuray: 0.7108434\n",
      "epoch: 58 - cost: 0.5056591 -MSE: 1.6592656522754912 train_accuray: 0.6807229\n",
      "epoch: 59 - cost: 0.59484303 -MSE: 1.492759428013736 train_accuray: 0.7108434\n",
      "epoch: 60 - cost: 0.5033729 -MSE: 1.676195275619573 train_accuray: 0.6807229\n",
      "epoch: 61 - cost: 0.592616 -MSE: 1.500081210097249 train_accuray: 0.7108434\n",
      "epoch: 62 - cost: 0.5011017 -MSE: 1.6930780947273392 train_accuray: 0.686747\n",
      "epoch: 63 - cost: 0.59039134 -MSE: 1.5074340018870647 train_accuray: 0.7108434\n",
      "epoch: 64 - cost: 0.4988452 -MSE: 1.709919463195467 train_accuray: 0.6927711\n",
      "epoch: 65 - cost: 0.5881684 -MSE: 1.5148207918363517 train_accuray: 0.7108434\n",
      "epoch: 66 - cost: 0.49660346 -MSE: 1.7267248166239846 train_accuray: 0.6927711\n",
      "epoch: 67 - cost: 0.5859476 -MSE: 1.522245555222371 train_accuray: 0.71686745\n",
      "epoch: 68 - cost: 0.49437612 -MSE: 1.7434987320769315 train_accuray: 0.6927711\n",
      "epoch: 69 - cost: 0.5837278 -MSE: 1.52971042480566 train_accuray: 0.71686745\n",
      "epoch: 70 - cost: 0.49216273 -MSE: 1.7602451651929938 train_accuray: 0.6927711\n",
      "epoch: 71 - cost: 0.5815094 -MSE: 1.5372192035416434 train_accuray: 0.71686745\n",
      "epoch: 72 - cost: 0.48996362 -MSE: 1.7769696647564488 train_accuray: 0.6927711\n",
      "epoch: 73 - cost: 0.5792928 -MSE: 1.544776205318914 train_accuray: 0.71686745\n",
      "epoch: 74 - cost: 0.48777837 -MSE: 1.7936777969326698 train_accuray: 0.7108434\n",
      "epoch: 75 - cost: 0.5770771 -MSE: 1.5523811383200818 train_accuray: 0.71686745\n",
      "epoch: 76 - cost: 0.4856062 -MSE: 1.8103673968598715 train_accuray: 0.7108434\n",
      "epoch: 77 - cost: 0.5748611 -MSE: 1.5600339546408888 train_accuray: 0.71686745\n",
      "epoch: 78 - cost: 0.4834465 -MSE: 1.8270432701519628 train_accuray: 0.7108434\n",
      "epoch: 79 - cost: 0.57264477 -MSE: 1.567736587639156 train_accuray: 0.71686745\n",
      "epoch: 80 - cost: 0.48129892 -MSE: 1.8437072175750897 train_accuray: 0.71686745\n",
      "epoch: 81 - cost: 0.57042754 -MSE: 1.5754902547350291 train_accuray: 0.71686745\n",
      "epoch: 82 - cost: 0.47916353 -MSE: 1.8603633967153137 train_accuray: 0.71686745\n",
      "epoch: 83 - cost: 0.56820995 -MSE: 1.5832972911038234 train_accuray: 0.71686745\n",
      "epoch: 84 - cost: 0.47703928 -MSE: 1.8770102179786594 train_accuray: 0.72289157\n",
      "epoch: 85 - cost: 0.5659902 -MSE: 1.5911539449092982 train_accuray: 0.71686745\n",
      "epoch: 86 - cost: 0.47492576 -MSE: 1.893648981199041 train_accuray: 0.72289157\n",
      "epoch: 87 - cost: 0.5637682 -MSE: 1.5990618552269287 train_accuray: 0.71686745\n",
      "epoch: 88 - cost: 0.47282204 -MSE: 1.9102785509669122 train_accuray: 0.7289157\n",
      "epoch: 89 - cost: 0.5615429 -MSE: 1.607018883096687 train_accuray: 0.71686745\n",
      "epoch: 90 - cost: 0.4707277 -MSE: 1.926898339434456 train_accuray: 0.7289157\n",
      "epoch: 91 - cost: 0.55931365 -MSE: 1.6150241036188273 train_accuray: 0.71686745\n",
      "epoch: 92 - cost: 0.46864244 -MSE: 1.943509382835575 train_accuray: 0.7289157\n",
      "epoch: 93 - cost: 0.5570808 -MSE: 1.623079007075812 train_accuray: 0.71686745\n",
      "epoch: 94 - cost: 0.4665652 -MSE: 1.9601091735125284 train_accuray: 0.7289157\n",
      "epoch: 95 - cost: 0.55484277 -MSE: 1.6311795369012474 train_accuray: 0.71686745\n",
      "epoch: 96 - cost: 0.4644957 -MSE: 1.9766969140849902 train_accuray: 0.7289157\n",
      "epoch: 97 - cost: 0.5525996 -MSE: 1.6393261696776056 train_accuray: 0.71686745\n",
      "epoch: 98 - cost: 0.462433 -MSE: 1.993269914690393 train_accuray: 0.73493975\n",
      "epoch: 99 - cost: 0.5503503 -MSE: 1.6475168728074294 train_accuray: 0.72289157\n",
      "epoch: 100 - cost: 0.4603769 -MSE: 2.0098266984245314 train_accuray: 0.73493975\n",
      "epoch: 101 - cost: 0.5480947 -MSE: 1.6557509865005797 train_accuray: 0.72289157\n",
      "epoch: 102 - cost: 0.45832655 -MSE: 2.026365497629472 train_accuray: 0.73493975\n",
      "epoch: 103 - cost: 0.54583186 -MSE: 1.6640252410589778 train_accuray: 0.72289157\n",
      "epoch: 104 - cost: 0.45628142 -MSE: 2.0428830177422643 train_accuray: 0.73493975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 105 - cost: 0.5435615 -MSE: 1.6723401439849268 train_accuray: 0.72289157\n",
      "epoch: 106 - cost: 0.45424092 -MSE: 2.0593769560893698 train_accuray: 0.73493975\n",
      "epoch: 107 - cost: 0.5412834 -MSE: 1.6806937875692785 train_accuray: 0.72289157\n",
      "epoch: 108 - cost: 0.4522046 -MSE: 2.075845089365668 train_accuray: 0.7409639\n",
      "epoch: 109 - cost: 0.53899705 -MSE: 1.6890841098655822 train_accuray: 0.72289157\n",
      "epoch: 110 - cost: 0.45017174 -MSE: 2.0922846532423076 train_accuray: 0.74698794\n",
      "epoch: 111 - cost: 0.5367015 -MSE: 1.6975093085381845 train_accuray: 0.72289157\n",
      "epoch: 112 - cost: 0.44814196 -MSE: 2.1086921146133553 train_accuray: 0.74698794\n",
      "epoch: 113 - cost: 0.5343972 -MSE: 1.7059699340299828 train_accuray: 0.72289157\n",
      "epoch: 114 - cost: 0.44611478 -MSE: 2.125065786989447 train_accuray: 0.75301206\n",
      "epoch: 115 - cost: 0.53208345 -MSE: 1.714463904211706 train_accuray: 0.72289157\n",
      "epoch: 116 - cost: 0.44408977 -MSE: 2.141402612158739 train_accuray: 0.75301206\n",
      "epoch: 117 - cost: 0.5297601 -MSE: 1.7229898674166624 train_accuray: 0.7289157\n",
      "epoch: 118 - cost: 0.44206637 -MSE: 2.1577000396858645 train_accuray: 0.7590361\n",
      "epoch: 119 - cost: 0.52742666 -MSE: 1.7315462384228335 train_accuray: 0.73493975\n",
      "epoch: 120 - cost: 0.44004416 -MSE: 2.173955253307812 train_accuray: 0.76506025\n",
      "epoch: 121 - cost: 0.52508324 -MSE: 1.7401331541104204 train_accuray: 0.73493975\n",
      "epoch: 122 - cost: 0.43802252 -MSE: 2.190165267564203 train_accuray: 0.76506025\n",
      "epoch: 123 - cost: 0.5227292 -MSE: 1.7487484836297642 train_accuray: 0.7409639\n",
      "epoch: 124 - cost: 0.43600148 -MSE: 2.206328962854125 train_accuray: 0.76506025\n",
      "epoch: 125 - cost: 0.5203649 -MSE: 1.7573922356187484 train_accuray: 0.7409639\n",
      "epoch: 126 - cost: 0.43398038 -MSE: 2.222443100195221 train_accuray: 0.7710843\n",
      "epoch: 127 - cost: 0.51799023 -MSE: 1.7660643595281367 train_accuray: 0.7409639\n",
      "epoch: 128 - cost: 0.43195903 -MSE: 2.238506429823565 train_accuray: 0.77710843\n",
      "epoch: 129 - cost: 0.5156052 -MSE: 1.7747637785486428 train_accuray: 0.7409639\n",
      "epoch: 130 - cost: 0.429937 -MSE: 2.2545157575666455 train_accuray: 0.77710843\n",
      "epoch: 131 - cost: 0.51320934 -MSE: 1.7834885729386027 train_accuray: 0.7409639\n",
      "epoch: 132 - cost: 0.4279141 -MSE: 2.270471032108059 train_accuray: 0.78313255\n",
      "epoch: 133 - cost: 0.510803 -MSE: 1.7922409237241808 train_accuray: 0.7409639\n",
      "epoch: 134 - cost: 0.42589012 -MSE: 2.286368809189564 train_accuray: 0.78313255\n",
      "epoch: 135 - cost: 0.5083862 -MSE: 1.8010180223661216 train_accuray: 0.7409639\n",
      "epoch: 136 - cost: 0.42386472 -MSE: 2.302208092807452 train_accuray: 0.78313255\n",
      "epoch: 137 - cost: 0.5059592 -MSE: 1.8098217198458078 train_accuray: 0.7409639\n",
      "epoch: 138 - cost: 0.4218375 -MSE: 2.317987523528246 train_accuray: 0.78313255\n",
      "epoch: 139 - cost: 0.5035216 -MSE: 1.8186504718055905 train_accuray: 0.7409639\n",
      "epoch: 140 - cost: 0.41980872 -MSE: 2.3337075074274702 train_accuray: 0.78313255\n",
      "epoch: 141 - cost: 0.5010744 -MSE: 1.8275054172696168 train_accuray: 0.7409639\n",
      "epoch: 142 - cost: 0.41777787 -MSE: 2.349365000039456 train_accuray: 0.78313255\n",
      "epoch: 143 - cost: 0.49861687 -MSE: 1.836385072796122 train_accuray: 0.74698794\n",
      "epoch: 144 - cost: 0.41574493 -MSE: 2.364959226792047 train_accuray: 0.79518074\n",
      "epoch: 145 - cost: 0.49614987 -MSE: 1.8452916580047098 train_accuray: 0.75301206\n",
      "epoch: 146 - cost: 0.41371012 -MSE: 2.3804918840805143 train_accuray: 0.79518074\n",
      "epoch: 147 - cost: 0.49367407 -MSE: 1.8542248350035033 train_accuray: 0.75301206\n",
      "epoch: 148 - cost: 0.41167307 -MSE: 2.395960363767768 train_accuray: 0.79518074\n",
      "epoch: 149 - cost: 0.49118906 -MSE: 1.8631845394621556 train_accuray: 0.75301206\n",
      "epoch: 150 - cost: 0.4096339 -MSE: 2.4113650976165144 train_accuray: 0.79518074\n",
      "epoch: 151 - cost: 0.4886956 -MSE: 1.8721714490216699 train_accuray: 0.75301206\n",
      "epoch: 152 - cost: 0.40759262 -MSE: 2.4267071928740016 train_accuray: 0.79518074\n",
      "epoch: 153 - cost: 0.4861942 -MSE: 1.8811870348162947 train_accuray: 0.75301206\n",
      "epoch: 154 - cost: 0.40554953 -MSE: 2.441985518927291 train_accuray: 0.79518074\n",
      "epoch: 155 - cost: 0.48368523 -MSE: 1.8902313942935283 train_accuray: 0.75301206\n",
      "epoch: 156 - cost: 0.40350443 -MSE: 2.457200421383844 train_accuray: 0.79518074\n",
      "epoch: 157 - cost: 0.48116922 -MSE: 1.8993053170436793 train_accuray: 0.75301206\n",
      "epoch: 158 - cost: 0.4014577 -MSE: 2.472353404603135 train_accuray: 0.79518074\n",
      "epoch: 159 - cost: 0.47864684 -MSE: 1.9084103183167536 train_accuray: 0.75301206\n",
      "epoch: 160 - cost: 0.3994092 -MSE: 2.487443528376465 train_accuray: 0.8012048\n",
      "epoch: 161 - cost: 0.47611833 -MSE: 1.9175464273094205 train_accuray: 0.75301206\n",
      "epoch: 162 - cost: 0.39735955 -MSE: 2.5024734251824086 train_accuray: 0.8012048\n",
      "epoch: 163 - cost: 0.4735849 -MSE: 1.9267147632358832 train_accuray: 0.75301206\n",
      "epoch: 164 - cost: 0.39530885 -MSE: 2.5174435448294283 train_accuray: 0.8012048\n",
      "epoch: 165 - cost: 0.471047 -MSE: 1.9359175254710672 train_accuray: 0.75301206\n",
      "epoch: 166 - cost: 0.39325747 -MSE: 2.532354301027014 train_accuray: 0.8072289\n",
      "epoch: 167 - cost: 0.46850553 -MSE: 1.9451556877428469 train_accuray: 0.75301206\n",
      "epoch: 168 - cost: 0.39120588 -MSE: 2.547210339425015 train_accuray: 0.8072289\n",
      "epoch: 169 - cost: 0.4659613 -MSE: 1.9544303402083647 train_accuray: 0.7590361\n",
      "epoch: 170 - cost: 0.38915423 -MSE: 2.562008781311396 train_accuray: 0.8072289\n",
      "epoch: 171 - cost: 0.4634147 -MSE: 1.9637425154194534 train_accuray: 0.7590361\n",
      "epoch: 172 - cost: 0.38710284 -MSE: 2.576752763499613 train_accuray: 0.813253\n",
      "epoch: 173 - cost: 0.46086672 -MSE: 1.9730926446711141 train_accuray: 0.7590361\n",
      "epoch: 174 - cost: 0.38505265 -MSE: 2.5914455453871903 train_accuray: 0.813253\n",
      "epoch: 175 - cost: 0.45831877 -MSE: 1.9824849000967173 train_accuray: 0.7590361\n",
      "epoch: 176 - cost: 0.38300386 -MSE: 2.6060879595488506 train_accuray: 0.813253\n",
      "epoch: 177 - cost: 0.45577145 -MSE: 1.9919186430990976 train_accuray: 0.7590361\n",
      "epoch: 178 - cost: 0.38095686 -MSE: 2.6206818933027143 train_accuray: 0.813253\n",
      "epoch: 179 - cost: 0.45322555 -MSE: 2.0013951670894565 train_accuray: 0.7590361\n",
      "epoch: 180 - cost: 0.3789124 -MSE: 2.6352301550119455 train_accuray: 0.813253\n",
      "epoch: 181 - cost: 0.4506824 -MSE: 2.010917834041762 train_accuray: 0.7590361\n",
      "epoch: 182 - cost: 0.37687108 -MSE: 2.6497356171515403 train_accuray: 0.813253\n",
      "epoch: 183 - cost: 0.4481431 -MSE: 2.0204868526039768 train_accuray: 0.7710843\n",
      "epoch: 184 - cost: 0.3748336 -MSE: 2.6641994576773476 train_accuray: 0.8192771\n",
      "epoch: 185 - cost: 0.44560853 -MSE: 2.030102914743086 train_accuray: 0.7710843\n",
      "epoch: 186 - cost: 0.37280035 -MSE: 2.67862468447753 train_accuray: 0.82530123\n",
      "epoch: 187 - cost: 0.44307953 -MSE: 2.0397689627360736 train_accuray: 0.7710843\n",
      "epoch: 188 - cost: 0.370772 -MSE: 2.693013893512441 train_accuray: 0.82530123\n",
      "epoch: 189 - cost: 0.44055703 -MSE: 2.049484695996726 train_accuray: 0.77710843\n",
      "epoch: 190 - cost: 0.36874884 -MSE: 2.7073665017107555 train_accuray: 0.8192771\n",
      "epoch: 191 - cost: 0.43804178 -MSE: 2.059251753040202 train_accuray: 0.78313255\n",
      "epoch: 192 - cost: 0.36673203 -MSE: 2.7216907341415113 train_accuray: 0.8192771\n",
      "epoch: 193 - cost: 0.435536 -MSE: 2.0690743155438462 train_accuray: 0.7891566\n",
      "epoch: 194 - cost: 0.36472234 -MSE: 2.7359876876376727 train_accuray: 0.8192771\n",
      "epoch: 195 - cost: 0.4330403 -MSE: 2.0789519317925405 train_accuray: 0.7891566\n",
      "epoch: 196 - cost: 0.36272028 -MSE: 2.7502599641800067 train_accuray: 0.8192771\n",
      "epoch: 197 - cost: 0.4305558 -MSE: 2.0888871570841596 train_accuray: 0.8012048\n",
      "epoch: 198 - cost: 0.36072657 -MSE: 2.764509878508911 train_accuray: 0.82530123\n",
      "epoch: 199 - cost: 0.42808345 -MSE: 2.09887930999 train_accuray: 0.8012048\n",
      "epoch: 200 - cost: 0.35874167 -MSE: 2.778740817393968 train_accuray: 0.82530123\n",
      "epoch: 201 - cost: 0.4256238 -MSE: 2.108929658996839 train_accuray: 0.8072289\n",
      "epoch: 202 - cost: 0.35676628 -MSE: 2.792953827725755 train_accuray: 0.82530123\n",
      "epoch: 203 - cost: 0.4231781 -MSE: 2.1190396409800223 train_accuray: 0.813253\n",
      "epoch: 204 - cost: 0.354801 -MSE: 2.807152995910028 train_accuray: 0.8313253\n",
      "epoch: 205 - cost: 0.42074728 -MSE: 2.1292104472443887 train_accuray: 0.813253\n",
      "epoch: 206 - cost: 0.35284632 -MSE: 2.8213403923047733 train_accuray: 0.8313253\n",
      "epoch: 207 - cost: 0.41833216 -MSE: 2.1394432191817025 train_accuray: 0.813253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 208 - cost: 0.3509032 -MSE: 2.835521485422259 train_accuray: 0.8313253\n",
      "epoch: 209 - cost: 0.41593385 -MSE: 2.149739831051017 train_accuray: 0.813253\n",
      "epoch: 210 - cost: 0.34897196 -MSE: 2.849695732817843 train_accuray: 0.8313253\n",
      "epoch: 211 - cost: 0.4135531 -MSE: 2.160099226139274 train_accuray: 0.813253\n",
      "epoch: 212 - cost: 0.34705317 -MSE: 2.8638680998342627 train_accuray: 0.8373494\n",
      "epoch: 213 - cost: 0.41119054 -MSE: 2.1705243978198157 train_accuray: 0.813253\n",
      "epoch: 214 - cost: 0.34514767 -MSE: 2.8780419603135527 train_accuray: 0.8373494\n",
      "epoch: 215 - cost: 0.40884745 -MSE: 2.181015557966894 train_accuray: 0.813253\n",
      "epoch: 216 - cost: 0.3432556 -MSE: 2.8922173077777384 train_accuray: 0.8373494\n",
      "epoch: 217 - cost: 0.4065241 -MSE: 2.191572675134811 train_accuray: 0.813253\n",
      "epoch: 218 - cost: 0.34137747 -MSE: 2.9063983417421264 train_accuray: 0.8373494\n",
      "epoch: 219 - cost: 0.40422115 -MSE: 2.202196534708861 train_accuray: 0.8192771\n",
      "epoch: 220 - cost: 0.33951372 -MSE: 2.920586947718855 train_accuray: 0.8373494\n",
      "epoch: 221 - cost: 0.40193862 -MSE: 2.2128860205607155 train_accuray: 0.8192771\n",
      "epoch: 222 - cost: 0.3376645 -MSE: 2.9347831924774277 train_accuray: 0.8433735\n",
      "epoch: 223 - cost: 0.39967728 -MSE: 2.22364330657168 train_accuray: 0.8192771\n",
      "epoch: 224 - cost: 0.33583015 -MSE: 2.9489904540245586 train_accuray: 0.8493976\n",
      "epoch: 225 - cost: 0.3974373 -MSE: 2.2344674641400424 train_accuray: 0.8192771\n",
      "epoch: 226 - cost: 0.33401114 -MSE: 2.9632134194322637 train_accuray: 0.8493976\n",
      "epoch: 227 - cost: 0.3952195 -MSE: 2.245360401836708 train_accuray: 0.82530123\n",
      "epoch: 228 - cost: 0.33220777 -MSE: 2.977451742219739 train_accuray: 0.8493976\n",
      "epoch: 229 - cost: 0.3930243 -MSE: 2.2563233230575643 train_accuray: 0.82530123\n",
      "epoch: 230 - cost: 0.33042046 -MSE: 2.9917091472578283 train_accuray: 0.8493976\n",
      "epoch: 231 - cost: 0.39085203 -MSE: 2.267354270221747 train_accuray: 0.82530123\n",
      "epoch: 232 - cost: 0.32864907 -MSE: 3.0059839545057243 train_accuray: 0.85542166\n",
      "epoch: 233 - cost: 0.38870212 -MSE: 2.2784527160233887 train_accuray: 0.82530123\n",
      "epoch: 234 - cost: 0.32689372 -MSE: 3.020278363093302 train_accuray: 0.85542166\n",
      "epoch: 235 - cost: 0.38657498 -MSE: 2.2896191102463286 train_accuray: 0.82530123\n",
      "epoch: 236 - cost: 0.32515448 -MSE: 3.034592963969044 train_accuray: 0.8614458\n",
      "epoch: 237 - cost: 0.38447028 -MSE: 2.3008529544948715 train_accuray: 0.8313253\n",
      "epoch: 238 - cost: 0.3234312 -MSE: 3.0489308192087217 train_accuray: 0.8614458\n",
      "epoch: 239 - cost: 0.38238803 -MSE: 2.312154492053285 train_accuray: 0.8313253\n",
      "epoch: 240 - cost: 0.32172403 -MSE: 3.063289667173803 train_accuray: 0.8614458\n",
      "epoch: 241 - cost: 0.38032824 -MSE: 2.323523038885841 train_accuray: 0.8313253\n",
      "epoch: 242 - cost: 0.32003298 -MSE: 3.0776725596736365 train_accuray: 0.8674699\n",
      "epoch: 243 - cost: 0.3782908 -MSE: 2.3349604698380024 train_accuray: 0.8373494\n",
      "epoch: 244 - cost: 0.3183582 -MSE: 3.0920810006412713 train_accuray: 0.8674699\n",
      "epoch: 245 - cost: 0.3762758 -MSE: 2.3464650539098093 train_accuray: 0.8373494\n",
      "epoch: 246 - cost: 0.31669942 -MSE: 3.1065134769128764 train_accuray: 0.87349397\n",
      "epoch: 247 - cost: 0.37428284 -MSE: 2.358035650315869 train_accuray: 0.8373494\n",
      "epoch: 248 - cost: 0.31505662 -MSE: 3.120970701041969 train_accuray: 0.87349397\n",
      "epoch: 249 - cost: 0.37231138 -MSE: 2.3696718885039534 train_accuray: 0.8373494\n",
      "epoch: 250 - cost: 0.31342953 -MSE: 3.135451492848687 train_accuray: 0.87349397\n",
      "epoch: 251 - cost: 0.37036115 -MSE: 2.381373415351447 train_accuray: 0.8373494\n",
      "epoch: 252 - cost: 0.31181785 -MSE: 3.149957155816815 train_accuray: 0.87349397\n",
      "epoch: 253 - cost: 0.36843166 -MSE: 2.3931385157605494 train_accuray: 0.8373494\n",
      "epoch: 254 - cost: 0.31022152 -MSE: 3.164484997566016 train_accuray: 0.87349397\n",
      "epoch: 255 - cost: 0.36652246 -MSE: 2.4049664510205284 train_accuray: 0.8373494\n",
      "epoch: 256 - cost: 0.3086401 -MSE: 3.1790346736238924 train_accuray: 0.87349397\n",
      "epoch: 257 - cost: 0.364633 -MSE: 2.41685768444663 train_accuray: 0.8373494\n",
      "epoch: 258 - cost: 0.30707344 -MSE: 3.193607107481788 train_accuray: 0.87349397\n",
      "epoch: 259 - cost: 0.3627627 -MSE: 2.4288105336181394 train_accuray: 0.8373494\n",
      "epoch: 260 - cost: 0.30552125 -MSE: 3.2082008853021087 train_accuray: 0.87349397\n",
      "epoch: 261 - cost: 0.36091164 -MSE: 2.4408260464960696 train_accuray: 0.8373494\n",
      "epoch: 262 - cost: 0.3039835 -MSE: 3.222816439287302 train_accuray: 0.87349397\n",
      "epoch: 263 - cost: 0.3590791 -MSE: 2.452901927291882 train_accuray: 0.8373494\n",
      "epoch: 264 - cost: 0.30245987 -MSE: 3.2374511644635504 train_accuray: 0.87349397\n",
      "epoch: 265 - cost: 0.35726485 -MSE: 2.465038157445691 train_accuray: 0.8373494\n",
      "epoch: 266 - cost: 0.3009501 -MSE: 3.252105569602148 train_accuray: 0.87349397\n",
      "epoch: 267 - cost: 0.35546818 -MSE: 2.4772337243591758 train_accuray: 0.8373494\n",
      "epoch: 268 - cost: 0.29945377 -MSE: 3.266778936410211 train_accuray: 0.87349397\n",
      "epoch: 269 - cost: 0.35368854 -MSE: 2.48948726567546 train_accuray: 0.8433735\n",
      "epoch: 270 - cost: 0.29797053 -MSE: 3.2814662028253374 train_accuray: 0.87349397\n",
      "epoch: 271 - cost: 0.35192513 -MSE: 2.5017970064379944 train_accuray: 0.8433735\n",
      "epoch: 272 - cost: 0.29650012 -MSE: 3.2961703370580153 train_accuray: 0.87349397\n",
      "epoch: 273 - cost: 0.3501777 -MSE: 2.5141632344752187 train_accuray: 0.8433735\n",
      "epoch: 274 - cost: 0.29504216 -MSE: 3.310887606078131 train_accuray: 0.87349397\n",
      "epoch: 275 - cost: 0.3484455 -MSE: 2.5265849804139626 train_accuray: 0.8433735\n",
      "epoch: 276 - cost: 0.29359627 -MSE: 3.325617141837467 train_accuray: 0.87349397\n",
      "epoch: 277 - cost: 0.34672815 -MSE: 2.539062073408751 train_accuray: 0.8433735\n",
      "epoch: 278 - cost: 0.29216245 -MSE: 3.3403606926466503 train_accuray: 0.87349397\n",
      "epoch: 279 - cost: 0.34502557 -MSE: 2.5515937859900766 train_accuray: 0.8433735\n",
      "epoch: 280 - cost: 0.2907404 -MSE: 3.3551153175466077 train_accuray: 0.87349397\n",
      "epoch: 281 - cost: 0.34333718 -MSE: 2.5641804372989796 train_accuray: 0.8433735\n",
      "epoch: 282 - cost: 0.2893297 -MSE: 3.3698804692059707 train_accuray: 0.87349397\n",
      "epoch: 283 - cost: 0.34166244 -MSE: 2.5768200171073286 train_accuray: 0.8433735\n",
      "epoch: 284 - cost: 0.28793022 -MSE: 3.384656251559327 train_accuray: 0.87349397\n",
      "epoch: 285 - cost: 0.3400009 -MSE: 2.5895119332573846 train_accuray: 0.8433735\n",
      "epoch: 286 - cost: 0.2865415 -MSE: 3.3994385277363173 train_accuray: 0.87349397\n",
      "epoch: 287 - cost: 0.33835196 -MSE: 2.602256189978154 train_accuray: 0.8433735\n",
      "epoch: 288 - cost: 0.2851631 -MSE: 3.4142283591045204 train_accuray: 0.87349397\n",
      "epoch: 289 - cost: 0.33671513 -MSE: 2.6150524819480983 train_accuray: 0.8433735\n",
      "epoch: 290 - cost: 0.28379506 -MSE: 3.4290252120413385 train_accuray: 0.8795181\n",
      "epoch: 291 - cost: 0.33509028 -MSE: 2.627899271344992 train_accuray: 0.8433735\n",
      "epoch: 292 - cost: 0.282437 -MSE: 3.443826950413641 train_accuray: 0.8795181\n",
      "epoch: 293 - cost: 0.33347687 -MSE: 2.6407970125351974 train_accuray: 0.8433735\n",
      "epoch: 294 - cost: 0.28108874 -MSE: 3.4586362909931694 train_accuray: 0.8795181\n",
      "epoch: 295 - cost: 0.3318748 -MSE: 2.653746330524443 train_accuray: 0.8433735\n",
      "epoch: 296 - cost: 0.2797499 -MSE: 3.473449959845161 train_accuray: 0.8795181\n",
      "epoch: 297 - cost: 0.33028325 -MSE: 2.666745799744995 train_accuray: 0.8433735\n",
      "epoch: 298 - cost: 0.27842033 -MSE: 3.4882675752351173 train_accuray: 0.8795181\n",
      "epoch: 299 - cost: 0.32870233 -MSE: 2.6797962344037205 train_accuray: 0.8433735\n",
      "epoch: 300 - cost: 0.27709994 -MSE: 3.5030907602044645 train_accuray: 0.8795181\n",
      "epoch: 301 - cost: 0.3271315 -MSE: 2.6928964029724862 train_accuray: 0.8433735\n",
      "epoch: 302 - cost: 0.2757882 -MSE: 3.517916904358267 train_accuray: 0.8795181\n",
      "epoch: 303 - cost: 0.3255702 -MSE: 2.7060456742491357 train_accuray: 0.8433735\n",
      "epoch: 304 - cost: 0.27448478 -MSE: 3.532745179528476 train_accuray: 0.8795181\n",
      "epoch: 305 - cost: 0.3240181 -MSE: 2.7192457351427866 train_accuray: 0.8433735\n",
      "epoch: 306 - cost: 0.2731897 -MSE: 3.5475780097800973 train_accuray: 0.8795181\n",
      "epoch: 307 - cost: 0.32247487 -MSE: 2.73249486941901 train_accuray: 0.8433735\n",
      "epoch: 308 - cost: 0.27190268 -MSE: 3.5624145579007114 train_accuray: 0.8795181\n",
      "epoch: 309 - cost: 0.32094067 -MSE: 2.7457957756948717 train_accuray: 0.8433735\n",
      "epoch: 310 - cost: 0.2706236 -MSE: 3.5772548606312258 train_accuray: 0.8795181\n",
      "epoch: 311 - cost: 0.3194148 -MSE: 2.7591460368604763 train_accuray: 0.85542166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 312 - cost: 0.26935217 -MSE: 3.592098607718166 train_accuray: 0.88554215\n",
      "epoch: 313 - cost: 0.31789693 -MSE: 2.7725475119800027 train_accuray: 0.85542166\n",
      "epoch: 314 - cost: 0.26808807 -MSE: 3.606947153074473 train_accuray: 0.88554215\n",
      "epoch: 315 - cost: 0.31638685 -MSE: 2.785999063177187 train_accuray: 0.85542166\n",
      "epoch: 316 - cost: 0.2668312 -MSE: 3.6218002537603478 train_accuray: 0.88554215\n",
      "epoch: 317 - cost: 0.31488425 -MSE: 2.7995011451027563 train_accuray: 0.85542166\n",
      "epoch: 318 - cost: 0.2655812 -MSE: 3.636655483210422 train_accuray: 0.8915663\n",
      "epoch: 319 - cost: 0.31338865 -MSE: 2.8130544717246058 train_accuray: 0.85542166\n",
      "epoch: 320 - cost: 0.26433808 -MSE: 3.651517927018104 train_accuray: 0.8915663\n",
      "epoch: 321 - cost: 0.31190005 -MSE: 2.82665866618702 train_accuray: 0.85542166\n",
      "epoch: 322 - cost: 0.26310143 -MSE: 3.6663839852110764 train_accuray: 0.89759034\n",
      "epoch: 323 - cost: 0.31041783 -MSE: 2.8403154010091165 train_accuray: 0.85542166\n",
      "epoch: 324 - cost: 0.2618712 -MSE: 3.681257685284661 train_accuray: 0.89759034\n",
      "epoch: 325 - cost: 0.3089423 -MSE: 2.8540242654363124 train_accuray: 0.85542166\n",
      "epoch: 326 - cost: 0.26064742 -MSE: 3.6961390228047124 train_accuray: 0.89759034\n",
      "epoch: 327 - cost: 0.30747312 -MSE: 2.8677870887657284 train_accuray: 0.85542166\n",
      "epoch: 328 - cost: 0.2594298 -MSE: 3.7110269696802725 train_accuray: 0.89759034\n",
      "epoch: 329 - cost: 0.30600998 -MSE: 2.881602260892759 train_accuray: 0.85542166\n",
      "epoch: 330 - cost: 0.25821796 -MSE: 3.725924464443213 train_accuray: 0.89759034\n",
      "epoch: 331 - cost: 0.30455214 -MSE: 2.8954692330643117 train_accuray: 0.85542166\n",
      "epoch: 332 - cost: 0.2570117 -MSE: 3.740829947686742 train_accuray: 0.89759034\n",
      "epoch: 333 - cost: 0.30309963 -MSE: 2.909391535580919 train_accuray: 0.85542166\n",
      "epoch: 334 - cost: 0.25581092 -MSE: 3.7557451471691867 train_accuray: 0.89759034\n",
      "epoch: 335 - cost: 0.30165207 -MSE: 2.923366987615275 train_accuray: 0.85542166\n",
      "epoch: 336 - cost: 0.2546154 -MSE: 3.7706699715881267 train_accuray: 0.89759034\n",
      "epoch: 337 - cost: 0.30020922 -MSE: 2.9373967077630514 train_accuray: 0.85542166\n",
      "epoch: 338 - cost: 0.25342488 -MSE: 3.7856079085745913 train_accuray: 0.89759034\n",
      "epoch: 339 - cost: 0.29877096 -MSE: 2.9514817462564955 train_accuray: 0.85542166\n",
      "epoch: 340 - cost: 0.2522393 -MSE: 3.800557389686599 train_accuray: 0.89759034\n",
      "epoch: 341 - cost: 0.29733703 -MSE: 2.9656237079722985 train_accuray: 0.85542166\n",
      "epoch: 342 - cost: 0.25105858 -MSE: 3.8155219178627817 train_accuray: 0.89759034\n",
      "epoch: 343 - cost: 0.29590738 -MSE: 2.9798224192039684 train_accuray: 0.8614458\n",
      "epoch: 344 - cost: 0.24988267 -MSE: 3.8305029600573617 train_accuray: 0.89759034\n",
      "epoch: 345 - cost: 0.29448193 -MSE: 2.994080175023007 train_accuray: 0.8614458\n",
      "epoch: 346 - cost: 0.24871135 -MSE: 3.8455014750532106 train_accuray: 0.89759034\n",
      "epoch: 347 - cost: 0.29306057 -MSE: 3.008395269074441 train_accuray: 0.8614458\n",
      "epoch: 348 - cost: 0.24754457 -MSE: 3.860517114133625 train_accuray: 0.89759034\n",
      "epoch: 349 - cost: 0.29164296 -MSE: 3.022768818756702 train_accuray: 0.8614458\n",
      "epoch: 350 - cost: 0.24638188 -MSE: 3.8755521335739282 train_accuray: 0.89759034\n",
      "epoch: 351 - cost: 0.29022843 -MSE: 3.0372010598954184 train_accuray: 0.8614458\n",
      "epoch: 352 - cost: 0.24522327 -MSE: 3.890606098243522 train_accuray: 0.89759034\n",
      "epoch: 353 - cost: 0.288817 -MSE: 3.0516918116638743 train_accuray: 0.8614458\n",
      "epoch: 354 - cost: 0.24406818 -MSE: 3.905680458145471 train_accuray: 0.89759034\n",
      "epoch: 355 - cost: 0.28740788 -MSE: 3.0662423707589372 train_accuray: 0.8614458\n",
      "epoch: 356 - cost: 0.24291669 -MSE: 3.9207766847221084 train_accuray: 0.89759034\n",
      "epoch: 357 - cost: 0.28600147 -MSE: 3.0808533649826884 train_accuray: 0.8614458\n",
      "epoch: 358 - cost: 0.24176864 -MSE: 3.9358960731691766 train_accuray: 0.89759034\n",
      "epoch: 359 - cost: 0.2845971 -MSE: 3.095525777396838 train_accuray: 0.8674699\n",
      "epoch: 360 - cost: 0.24062373 -MSE: 3.9510417981405075 train_accuray: 0.89759034\n",
      "epoch: 361 - cost: 0.28319484 -MSE: 3.110261064708659 train_accuray: 0.8674699\n",
      "epoch: 362 - cost: 0.239482 -MSE: 3.966213540814052 train_accuray: 0.89759034\n",
      "epoch: 363 - cost: 0.28179482 -MSE: 3.1250601596988155 train_accuray: 0.87349397\n",
      "epoch: 364 - cost: 0.23834348 -MSE: 3.9814177514453193 train_accuray: 0.89759034\n",
      "epoch: 365 - cost: 0.28039688 -MSE: 3.1399245376020857 train_accuray: 0.87349397\n",
      "epoch: 366 - cost: 0.23720813 -MSE: 3.99665184043939 train_accuray: 0.89759034\n",
      "epoch: 367 - cost: 0.2790011 -MSE: 3.154854661339332 train_accuray: 0.87349397\n",
      "epoch: 368 - cost: 0.23607562 -MSE: 4.011919998803957 train_accuray: 0.89759034\n",
      "epoch: 369 - cost: 0.27760682 -MSE: 3.169850283863186 train_accuray: 0.87349397\n",
      "epoch: 370 - cost: 0.2349458 -MSE: 4.027218179325736 train_accuray: 0.89759034\n",
      "epoch: 371 - cost: 0.27621403 -MSE: 3.1849112650519835 train_accuray: 0.87349397\n",
      "epoch: 372 - cost: 0.23381865 -MSE: 4.042553027645822 train_accuray: 0.89759034\n",
      "epoch: 373 - cost: 0.2748225 -MSE: 3.2000382523276687 train_accuray: 0.87349397\n",
      "epoch: 374 - cost: 0.23269361 -MSE: 4.057922296805165 train_accuray: 0.89759034\n",
      "epoch: 375 - cost: 0.2734316 -MSE: 3.2152328710853246 train_accuray: 0.87349397\n",
      "epoch: 376 - cost: 0.2315706 -MSE: 4.073329945149418 train_accuray: 0.89759034\n",
      "epoch: 377 - cost: 0.27204102 -MSE: 3.2304942847711984 train_accuray: 0.87349397\n",
      "epoch: 378 - cost: 0.23044915 -MSE: 4.088773058423599 train_accuray: 0.89759034\n",
      "epoch: 379 - cost: 0.2706502 -MSE: 3.2458231209882045 train_accuray: 0.87349397\n",
      "epoch: 380 - cost: 0.22932918 -MSE: 4.104257345624318 train_accuray: 0.90361446\n",
      "epoch: 381 - cost: 0.26925918 -MSE: 3.2612230229275823 train_accuray: 0.87349397\n",
      "epoch: 382 - cost: 0.22821054 -MSE: 4.119781927899316 train_accuray: 0.90361446\n",
      "epoch: 383 - cost: 0.26786777 -MSE: 3.276692804716806 train_accuray: 0.87349397\n",
      "epoch: 384 - cost: 0.22709304 -MSE: 4.1353498742502 train_accuray: 0.9096386\n",
      "epoch: 385 - cost: 0.2664761 -MSE: 3.292234993687147 train_accuray: 0.8795181\n",
      "epoch: 386 - cost: 0.22597682 -MSE: 4.15096553823089 train_accuray: 0.9096386\n",
      "epoch: 387 - cost: 0.26508406 -MSE: 3.3078518214384 train_accuray: 0.8795181\n",
      "epoch: 388 - cost: 0.22486186 -MSE: 4.16663227694406 train_accuray: 0.9096386\n",
      "epoch: 389 - cost: 0.26369193 -MSE: 3.323545879155657 train_accuray: 0.8795181\n",
      "epoch: 390 - cost: 0.22374824 -MSE: 4.182350243188756 train_accuray: 0.9096386\n",
      "epoch: 391 - cost: 0.2622996 -MSE: 3.3393140402401698 train_accuray: 0.8795181\n",
      "epoch: 392 - cost: 0.22263566 -MSE: 4.198122107390775 train_accuray: 0.91566265\n",
      "epoch: 393 - cost: 0.2609069 -MSE: 3.3551598890537653 train_accuray: 0.8795181\n",
      "epoch: 394 - cost: 0.221524 -MSE: 4.213948175073516 train_accuray: 0.91566265\n",
      "epoch: 395 - cost: 0.2595133 -MSE: 3.3710825337418546 train_accuray: 0.8795181\n",
      "epoch: 396 - cost: 0.22041275 -MSE: 4.229827978765731 train_accuray: 0.91566265\n",
      "epoch: 397 - cost: 0.25811833 -MSE: 3.387080627679778 train_accuray: 0.8795181\n",
      "epoch: 398 - cost: 0.2193017 -MSE: 4.24576355832629 train_accuray: 0.91566265\n",
      "epoch: 399 - cost: 0.25672156 -MSE: 3.4031557613547117 train_accuray: 0.8795181\n",
      "epoch: 400 - cost: 0.21819018 -MSE: 4.261751664081326 train_accuray: 0.91566265\n",
      "epoch: 401 - cost: 0.25532177 -MSE: 3.419307286338373 train_accuray: 0.8795181\n",
      "epoch: 402 - cost: 0.2170779 -MSE: 4.277795110813792 train_accuray: 0.91566265\n",
      "epoch: 403 - cost: 0.25391898 -MSE: 3.4355366985213607 train_accuray: 0.8795181\n",
      "epoch: 404 - cost: 0.21596447 -MSE: 4.293897261282332 train_accuray: 0.91566265\n",
      "epoch: 405 - cost: 0.2525126 -MSE: 3.451848080134189 train_accuray: 0.8795181\n",
      "epoch: 406 - cost: 0.21484986 -MSE: 4.310059126034615 train_accuray: 0.91566265\n",
      "epoch: 407 - cost: 0.25110278 -MSE: 3.468242853279874 train_accuray: 0.8795181\n",
      "epoch: 408 - cost: 0.21373397 -MSE: 4.326286199770958 train_accuray: 0.91566265\n",
      "epoch: 409 - cost: 0.2496897 -MSE: 3.484722748783157 train_accuray: 0.88554215\n",
      "epoch: 410 - cost: 0.212617 -MSE: 4.342582956821052 train_accuray: 0.91566265\n",
      "epoch: 411 - cost: 0.24827385 -MSE: 3.501292389666808 train_accuray: 0.88554215\n",
      "epoch: 412 - cost: 0.21149915 -MSE: 4.358951954991783 train_accuray: 0.91566265\n",
      "epoch: 413 - cost: 0.24685536 -MSE: 3.5179531132147694 train_accuray: 0.88554215\n",
      "epoch: 414 - cost: 0.21038055 -MSE: 4.375396496074643 train_accuray: 0.91566265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 415 - cost: 0.24543482 -MSE: 3.534706968433488 train_accuray: 0.8915663\n",
      "epoch: 416 - cost: 0.20926116 -MSE: 4.391921259403487 train_accuray: 0.91566265\n",
      "epoch: 417 - cost: 0.2440119 -MSE: 3.551552411142875 train_accuray: 0.8915663\n",
      "epoch: 418 - cost: 0.20814075 -MSE: 4.408523957310773 train_accuray: 0.91566265\n",
      "epoch: 419 - cost: 0.24258621 -MSE: 3.568488986987584 train_accuray: 0.8915663\n",
      "epoch: 420 - cost: 0.20701873 -MSE: 4.425202662883992 train_accuray: 0.91566265\n",
      "epoch: 421 - cost: 0.24115682 -MSE: 3.5855175423352184 train_accuray: 0.8915663\n",
      "epoch: 422 - cost: 0.20589451 -MSE: 4.441956168468431 train_accuray: 0.91566265\n",
      "epoch: 423 - cost: 0.23972268 -MSE: 3.602635149925579 train_accuray: 0.8915663\n",
      "epoch: 424 - cost: 0.20476723 -MSE: 4.458783196602189 train_accuray: 0.91566265\n",
      "epoch: 425 - cost: 0.2382826 -MSE: 3.619843891872713 train_accuray: 0.8915663\n",
      "epoch: 426 - cost: 0.203636 -MSE: 4.475682796399618 train_accuray: 0.91566265\n",
      "epoch: 427 - cost: 0.23683526 -MSE: 3.637144090746379 train_accuray: 0.8915663\n",
      "epoch: 428 - cost: 0.20250024 -MSE: 4.4926547558712135 train_accuray: 0.91566265\n",
      "epoch: 429 - cost: 0.2353801 -MSE: 3.654538095982794 train_accuray: 0.8915663\n",
      "epoch: 430 - cost: 0.20135935 -MSE: 4.509702233023073 train_accuray: 0.91566265\n",
      "epoch: 431 - cost: 0.23391655 -MSE: 3.672032083594805 train_accuray: 0.8915663\n",
      "epoch: 432 - cost: 0.2002135 -MSE: 4.526831732870033 train_accuray: 0.91566265\n",
      "epoch: 433 - cost: 0.23244536 -MSE: 3.689632167424041 train_accuray: 0.8915663\n",
      "epoch: 434 - cost: 0.19906276 -MSE: 4.544050280264116 train_accuray: 0.91566265\n",
      "epoch: 435 - cost: 0.23096685 -MSE: 3.7073428508339017 train_accuray: 0.8915663\n",
      "epoch: 436 - cost: 0.1979078 -MSE: 4.561366055092354 train_accuray: 0.91566265\n",
      "epoch: 437 - cost: 0.22948258 -MSE: 3.7251699605213817 train_accuray: 0.8915663\n",
      "epoch: 438 - cost: 0.19674958 -MSE: 4.578790787309503 train_accuray: 0.91566265\n",
      "epoch: 439 - cost: 0.22799419 -MSE: 3.7431189683389334 train_accuray: 0.8915663\n",
      "epoch: 440 - cost: 0.19558851 -MSE: 4.59632601154738 train_accuray: 0.91566265\n",
      "epoch: 441 - cost: 0.22650227 -MSE: 3.761190422706097 train_accuray: 0.8915663\n",
      "epoch: 442 - cost: 0.19442494 -MSE: 4.613977330819127 train_accuray: 0.91566265\n",
      "epoch: 443 - cost: 0.22500724 -MSE: 3.7793836760396458 train_accuray: 0.8915663\n",
      "epoch: 444 - cost: 0.19325893 -MSE: 4.631744633684242 train_accuray: 0.91566265\n",
      "epoch: 445 - cost: 0.22350895 -MSE: 3.797697401674545 train_accuray: 0.8915663\n",
      "epoch: 446 - cost: 0.19208986 -MSE: 4.649621182618948 train_accuray: 0.91566265\n",
      "epoch: 447 - cost: 0.2220063 -MSE: 3.8161278827045186 train_accuray: 0.8915663\n",
      "epoch: 448 - cost: 0.19091663 -MSE: 4.667603310158442 train_accuray: 0.91566265\n",
      "epoch: 449 - cost: 0.22049718 -MSE: 3.834671631146694 train_accuray: 0.8915663\n",
      "epoch: 450 - cost: 0.1897373 -MSE: 4.685677219037973 train_accuray: 0.91566265\n",
      "epoch: 451 - cost: 0.2189785 -MSE: 3.8533228224553784 train_accuray: 0.8915663\n",
      "epoch: 452 - cost: 0.18855001 -MSE: 4.703835566267532 train_accuray: 0.92168677\n",
      "epoch: 453 - cost: 0.21744725 -MSE: 3.8720823811334406 train_accuray: 0.8915663\n",
      "epoch: 454 - cost: 0.18735255 -MSE: 4.722070779784405 train_accuray: 0.92168677\n",
      "epoch: 455 - cost: 0.21590047 -MSE: 3.8909536626192716 train_accuray: 0.8915663\n",
      "epoch: 456 - cost: 0.1861431 -MSE: 4.740384364950654 train_accuray: 0.92168677\n",
      "epoch: 457 - cost: 0.21433601 -MSE: 3.909943380370245 train_accuray: 0.8915663\n",
      "epoch: 458 - cost: 0.1849207 -MSE: 4.758779205437616 train_accuray: 0.92168677\n",
      "epoch: 459 - cost: 0.21275309 -MSE: 3.929063783798149 train_accuray: 0.8915663\n",
      "epoch: 460 - cost: 0.18368505 -MSE: 4.777263761092358 train_accuray: 0.92168677\n",
      "epoch: 461 - cost: 0.21115209 -MSE: 3.948328391155241 train_accuray: 0.8915663\n",
      "epoch: 462 - cost: 0.18243687 -MSE: 4.79586001035626 train_accuray: 0.92168677\n",
      "epoch: 463 - cost: 0.20953535 -MSE: 3.9677552477957336 train_accuray: 0.8915663\n",
      "epoch: 464 - cost: 0.18117803 -MSE: 4.814590633598685 train_accuray: 0.92168677\n",
      "epoch: 465 - cost: 0.20790663 -MSE: 3.9873575644075623 train_accuray: 0.89759034\n",
      "epoch: 466 - cost: 0.17991138 -MSE: 4.833479083658272 train_accuray: 0.92168677\n",
      "epoch: 467 - cost: 0.20627098 -MSE: 4.007153219752515 train_accuray: 0.89759034\n",
      "epoch: 468 - cost: 0.17864037 -MSE: 4.8525567638569385 train_accuray: 0.92168677\n",
      "epoch: 469 - cost: 0.20463406 -MSE: 4.0271518060419 train_accuray: 0.90361446\n",
      "epoch: 470 - cost: 0.17736828 -MSE: 4.871839404758544 train_accuray: 0.92771083\n",
      "epoch: 471 - cost: 0.20300086 -MSE: 4.047351703742189 train_accuray: 0.9096386\n",
      "epoch: 472 - cost: 0.17609797 -MSE: 4.8913395770452714 train_accuray: 0.92771083\n",
      "epoch: 473 - cost: 0.20137525 -MSE: 4.067749788372824 train_accuray: 0.9096386\n",
      "epoch: 474 - cost: 0.17483158 -MSE: 4.91106004387338 train_accuray: 0.92771083\n",
      "epoch: 475 - cost: 0.19975959 -MSE: 4.088334973963674 train_accuray: 0.9096386\n",
      "epoch: 476 - cost: 0.17356928 -MSE: 4.930986464898216 train_accuray: 0.92771083\n",
      "epoch: 477 - cost: 0.19815302 -MSE: 4.1090873414770765 train_accuray: 0.9096386\n",
      "epoch: 478 - cost: 0.1723096 -MSE: 4.951095401008257 train_accuray: 0.92771083\n",
      "epoch: 479 - cost: 0.19655159 -MSE: 4.12998403514939 train_accuray: 0.9096386\n",
      "epoch: 480 - cost: 0.17104867 -MSE: 4.9713442396179435 train_accuray: 0.93373495\n",
      "epoch: 481 - cost: 0.19494836 -MSE: 4.1510014921324165 train_accuray: 0.9096386\n",
      "epoch: 482 - cost: 0.16978078 -MSE: 4.991688776911336 train_accuray: 0.93373495\n",
      "epoch: 483 - cost: 0.1933333 -MSE: 4.172120602367992 train_accuray: 0.9096386\n",
      "epoch: 484 - cost: 0.16849795 -MSE: 5.012073236854953 train_accuray: 0.93373495\n",
      "epoch: 485 - cost: 0.19169338 -MSE: 4.193327819549483 train_accuray: 0.9096386\n",
      "epoch: 486 - cost: 0.16719133 -MSE: 5.032445964754511 train_accuray: 0.93373495\n",
      "epoch: 487 - cost: 0.19001485 -MSE: 4.214621782502023 train_accuray: 0.91566265\n",
      "epoch: 488 - cost: 0.16585095 -MSE: 5.052764519421705 train_accuray: 0.93373495\n",
      "epoch: 489 - cost: 0.18828377 -MSE: 4.236018128422181 train_accuray: 0.91566265\n",
      "epoch: 490 - cost: 0.16446818 -MSE: 5.0730058950855526 train_accuray: 0.93373495\n",
      "epoch: 491 - cost: 0.18648818 -MSE: 4.257545241008519 train_accuray: 0.91566265\n",
      "epoch: 492 - cost: 0.16303569 -MSE: 5.093165218609575 train_accuray: 0.93373495\n",
      "epoch: 493 - cost: 0.18461978 -MSE: 4.279256014532558 train_accuray: 0.91566265\n",
      "epoch: 494 - cost: 0.1615495 -MSE: 5.113271115565427 train_accuray: 0.93373495\n",
      "epoch: 495 - cost: 0.18267593 -MSE: 4.3012170318809275 train_accuray: 0.91566265\n",
      "epoch: 496 - cost: 0.16000982 -MSE: 5.1333915540761845 train_accuray: 0.93373495\n",
      "epoch: 497 - cost: 0.18066113 -MSE: 4.323504925851337 train_accuray: 0.91566265\n",
      "epoch: 498 - cost: 0.15842223 -MSE: 5.153623543896904 train_accuray: 0.93373495\n",
      "epoch: 499 - cost: 0.17858821 -MSE: 4.34621040791188 train_accuray: 0.91566265\n",
      "epoch: 500 - cost: 0.15679832 -MSE: 5.174102640405756 train_accuray: 0.93373495\n",
      "epoch: 501 - cost: 0.17647897 -MSE: 4.369414640000179 train_accuray: 0.91566265\n",
      "epoch: 502 - cost: 0.15515555 -MSE: 5.194987251513661 train_accuray: 0.93373495\n",
      "epoch: 503 - cost: 0.17436357 -MSE: 4.393186921713754 train_accuray: 0.92168677\n",
      "epoch: 504 - cost: 0.15351735 -MSE: 5.216459591578153 train_accuray: 0.93373495\n",
      "epoch: 505 - cost: 0.17227964 -MSE: 4.4175734076830775 train_accuray: 0.92168677\n",
      "epoch: 506 - cost: 0.1519117 -MSE: 5.23869828591686 train_accuray: 0.93373495\n",
      "epoch: 507 - cost: 0.17027058 -MSE: 4.442583771696779 train_accuray: 0.92168677\n",
      "epoch: 508 - cost: 0.15037024 -MSE: 5.2618761024549 train_accuray: 0.93373495\n",
      "epoch: 509 - cost: 0.16838326 -MSE: 4.468185122470189 train_accuray: 0.92168677\n",
      "epoch: 510 - cost: 0.14892623 -MSE: 5.2861338228329275 train_accuray: 0.93373495\n",
      "epoch: 511 - cost: 0.16666518 -MSE: 4.494295938791303 train_accuray: 0.92168677\n",
      "epoch: 512 - cost: 0.14761317 -MSE: 5.311576371775688 train_accuray: 0.939759\n",
      "epoch: 513 - cost: 0.16516297 -MSE: 4.520789345062137 train_accuray: 0.92168677\n",
      "epoch: 514 - cost: 0.14646277 -MSE: 5.338254931475829 train_accuray: 0.939759\n",
      "epoch: 515 - cost: 0.16391893 -MSE: 4.547491180010097 train_accuray: 0.92168677\n",
      "epoch: 516 - cost: 0.14550388 -MSE: 5.3661681627830475 train_accuray: 0.939759\n",
      "epoch: 517 - cost: 0.16297042 -MSE: 4.57419802096287 train_accuray: 0.92168677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 518 - cost: 0.14476065 -MSE: 5.395245988337268 train_accuray: 0.939759\n",
      "epoch: 519 - cost: 0.16234721 -MSE: 4.600676438851615 train_accuray: 0.92168677\n",
      "epoch: 520 - cost: 0.14425139 -MSE: 5.425354580839484 train_accuray: 0.939759\n",
      "epoch: 521 - cost: 0.16206959 -MSE: 4.626676389283611 train_accuray: 0.92168677\n",
      "epoch: 522 - cost: 0.14398645 -MSE: 5.456293601763969 train_accuray: 0.93373495\n",
      "epoch: 523 - cost: 0.16214606 -MSE: 4.651949380478295 train_accuray: 0.92168677\n",
      "epoch: 524 - cost: 0.14396665 -MSE: 5.48778818394908 train_accuray: 0.93373495\n",
      "epoch: 525 - cost: 0.16256976 -MSE: 4.67623971831785 train_accuray: 0.92168677\n",
      "epoch: 526 - cost: 0.14417936 -MSE: 5.519474689212868 train_accuray: 0.93373495\n",
      "epoch: 527 - cost: 0.16331159 -MSE: 4.699293339111034 train_accuray: 0.92168677\n",
      "epoch: 528 - cost: 0.14459269 -MSE: 5.550868901932624 train_accuray: 0.93373495\n",
      "epoch: 529 - cost: 0.16430983 -MSE: 4.720851777087936 train_accuray: 0.91566265\n",
      "epoch: 530 - cost: 0.1451462 -MSE: 5.5813154685291755 train_accuray: 0.93373495\n",
      "epoch: 531 - cost: 0.16545288 -MSE: 4.740620692106011 train_accuray: 0.91566265\n",
      "epoch: 532 - cost: 0.14573579 -MSE: 5.609893850854277 train_accuray: 0.93373495\n",
      "epoch: 533 - cost: 0.16655183 -MSE: 4.758252414373441 train_accuray: 0.91566265\n",
      "epoch: 534 - cost: 0.14619121 -MSE: 5.635285753653821 train_accuray: 0.93373495\n",
      "epoch: 535 - cost: 0.16730234 -MSE: 4.7733129594871615 train_accuray: 0.91566265\n",
      "epoch: 536 - cost: 0.1462452 -MSE: 5.655581522272025 train_accuray: 0.93373495\n",
      "epoch: 537 - cost: 0.16723478 -MSE: 4.785264676263951 train_accuray: 0.91566265\n",
      "epoch: 538 - cost: 0.14550014 -MSE: 5.668104541515478 train_accuray: 0.93373495\n",
      "epoch: 539 - cost: 0.16567418 -MSE: 4.793594628788851 train_accuray: 0.91566265\n",
      "epoch: 540 - cost: 0.14341487 -MSE: 5.669411990877158 train_accuray: 0.93373495\n",
      "epoch: 541 - cost: 0.16176626 -MSE: 4.798278321664029 train_accuray: 0.92168677\n",
      "epoch: 542 - cost: 0.13937095 -MSE: 5.655930061154669 train_accuray: 0.93373495\n",
      "epoch: 543 - cost: 0.15468594 -MSE: 4.800957332418481 train_accuray: 0.92771083\n",
      "epoch: 544 - cost: 0.13291287 -MSE: 5.626040087650968 train_accuray: 0.9518072\n",
      "epoch: 545 - cost: 0.14415245 -MSE: 4.806897589293687 train_accuray: 0.939759\n",
      "epoch: 546 - cost: 0.12419561 -MSE: 5.58388819765632 train_accuray: 0.9518072\n",
      "epoch: 547 - cost: 0.13110128 -MSE: 4.8262095435495125 train_accuray: 0.94578314\n",
      "epoch: 548 - cost: 0.114344105 -MSE: 5.5424260272630494 train_accuray: 0.9518072\n",
      "epoch: 549 - cost: 0.11776095 -MSE: 4.870397948320227 train_accuray: 0.9518072\n",
      "epoch: 550 - cost: 0.105086856 -MSE: 5.519414211226392 train_accuray: 0.9578313\n",
      "epoch: 551 - cost: 0.1064652 -MSE: 4.943469220539098 train_accuray: 0.96385545\n",
      "epoch: 552 - cost: 0.09768624 -MSE: 5.526033813551718 train_accuray: 0.96385545\n",
      "epoch: 553 - cost: 0.09822971 -MSE: 5.037340266511235 train_accuray: 0.96385545\n",
      "epoch: 554 - cost: 0.09237625 -MSE: 5.561000821229131 train_accuray: 0.9698795\n",
      "epoch: 555 - cost: 0.09272857 -MSE: 5.138512297352394 train_accuray: 0.9698795\n",
      "epoch: 556 - cost: 0.088765055 -MSE: 5.616080057204104 train_accuray: 0.9698795\n",
      "epoch: 557 - cost: 0.08919711 -MSE: 5.236480537834426 train_accuray: 0.9698795\n",
      "epoch: 558 - cost: 0.086380914 -MSE: 5.683196904311513 train_accuray: 0.9698795\n",
      "epoch: 559 - cost: 0.08701908 -MSE: 5.325811792782041 train_accuray: 0.97590363\n",
      "epoch: 560 - cost: 0.08489842 -MSE: 5.757110853963609 train_accuray: 0.9698795\n",
      "epoch: 561 - cost: 0.08584892 -MSE: 5.404474747904864 train_accuray: 0.97590363\n",
      "epoch: 562 - cost: 0.08416296 -MSE: 5.835235921235453 train_accuray: 0.9698795\n",
      "epoch: 563 - cost: 0.08557376 -MSE: 5.471956759015798 train_accuray: 0.97590363\n",
      "epoch: 564 - cost: 0.084170654 -MSE: 5.916846979281822 train_accuray: 0.9698795\n",
      "epoch: 565 - cost: 0.08628277 -MSE: 5.52820600988202 train_accuray: 0.9698795\n",
      "epoch: 566 - cost: 0.085067965 -MSE: 6.00255787674843 train_accuray: 0.9698795\n",
      "epoch: 567 - cost: 0.088293366 -MSE: 5.573235534808069 train_accuray: 0.9698795\n",
      "epoch: 568 - cost: 0.087197915 -MSE: 6.094282339161518 train_accuray: 0.9698795\n",
      "epoch: 569 - cost: 0.09227285 -MSE: 5.607287848332954 train_accuray: 0.96385545\n",
      "epoch: 570 - cost: 0.09123976 -MSE: 6.196003762766217 train_accuray: 0.96385545\n",
      "epoch: 571 - cost: 0.09957614 -MSE: 5.631777182736044 train_accuray: 0.96385545\n",
      "epoch: 572 - cost: 0.09862094 -MSE: 6.316895499289553 train_accuray: 0.9578313\n",
      "epoch: 573 - cost: 0.11330377 -MSE: 5.652735291301271 train_accuray: 0.9518072\n",
      "epoch: 574 - cost: 0.113061294 -MSE: 6.483876924712728 train_accuray: 0.9578313\n",
      "epoch: 575 - cost: 0.14287849 -MSE: 5.697094101357428 train_accuray: 0.92771083\n",
      "epoch: 576 - cost: 0.1490924 -MSE: 6.807761015129461 train_accuray: 0.94578314\n",
      "epoch: 577 - cost: 0.23691198 -MSE: 5.938652617271761 train_accuray: 0.8915663\n",
      "epoch: 578 - cost: 0.31752503 -MSE: 8.017619407657337 train_accuray: 0.8433735\n",
      "epoch: 579 - cost: 0.98100317 -MSE: 8.51243362143254 train_accuray: 0.6626506\n",
      "epoch: 580 - cost: 1.9419721 -MSE: 15.818332129133811 train_accuray: 0.4939759\n",
      "epoch: 581 - cost: 2.7184806 -MSE: 13.706174784619375 train_accuray: 0.560241\n",
      "epoch: 582 - cost: 0.7090183 -MSE: 2.7753815400217623 train_accuray: 0.5421687\n",
      "epoch: 583 - cost: 0.54978675 -MSE: 2.3533786488820576 train_accuray: 0.6686747\n",
      "epoch: 584 - cost: 0.4419642 -MSE: 2.108279278069103 train_accuray: 0.8192771\n",
      "epoch: 585 - cost: 0.36374658 -MSE: 1.937765596193171 train_accuray: 0.8192771\n",
      "epoch: 586 - cost: 0.28728482 -MSE: 1.957053577866007 train_accuray: 0.8674699\n",
      "epoch: 587 - cost: 0.2319199 -MSE: 2.131391071137563 train_accuray: 0.92771083\n",
      "epoch: 588 - cost: 0.19460244 -MSE: 2.2943620995412832 train_accuray: 0.93373495\n",
      "epoch: 589 - cost: 0.17262852 -MSE: 2.577908984048048 train_accuray: 0.9578313\n",
      "epoch: 590 - cost: 0.15794581 -MSE: 2.7317428908772516 train_accuray: 0.94578314\n",
      "epoch: 591 - cost: 0.14727828 -MSE: 2.977780938669392 train_accuray: 0.94578314\n",
      "epoch: 592 - cost: 0.13910532 -MSE: 3.1197333342227695 train_accuray: 0.94578314\n",
      "epoch: 593 - cost: 0.13244575 -MSE: 3.3221483827943206 train_accuray: 0.9518072\n",
      "epoch: 594 - cost: 0.12677085 -MSE: 3.455108562284801 train_accuray: 0.9578313\n",
      "epoch: 595 - cost: 0.12178355 -MSE: 3.624063712078848 train_accuray: 0.9578313\n",
      "epoch: 596 - cost: 0.117279455 -MSE: 3.748513592253417 train_accuray: 0.9698795\n",
      "epoch: 597 - cost: 0.11316698 -MSE: 3.8932755405041646 train_accuray: 0.9698795\n",
      "epoch: 598 - cost: 0.109368004 -MSE: 4.009404227685264 train_accuray: 0.9698795\n",
      "epoch: 599 - cost: 0.105853334 -MSE: 4.136320253844353 train_accuray: 0.9698795\n",
      "epoch: 600 - cost: 0.10259242 -MSE: 4.244420082938829 train_accuray: 0.9698795\n",
      "epoch: 601 - cost: 0.09957394 -MSE: 4.357555205439355 train_accuray: 0.9698795\n",
      "epoch: 602 - cost: 0.09678192 -MSE: 4.457923771048954 train_accuray: 0.9698795\n",
      "epoch: 603 - cost: 0.094206534 -MSE: 4.55986119486185 train_accuray: 0.9698795\n",
      "epoch: 604 - cost: 0.09183353 -MSE: 4.652765437844302 train_accuray: 0.9698795\n",
      "epoch: 605 - cost: 0.08965031 -MSE: 4.74520064277446 train_accuray: 0.97590363\n",
      "epoch: 606 - cost: 0.08764186 -MSE: 4.8309378344469955 train_accuray: 0.97590363\n",
      "epoch: 607 - cost: 0.08579403 -MSE: 4.915087120662788 train_accuray: 0.97590363\n",
      "epoch: 608 - cost: 0.084092245 -MSE: 4.994043129520637 train_accuray: 0.97590363\n",
      "epoch: 609 - cost: 0.08252296 -MSE: 5.070880457515521 train_accuray: 0.97590363\n",
      "epoch: 610 - cost: 0.081073195 -MSE: 5.14354410045072 train_accuray: 0.97590363\n",
      "epoch: 611 - cost: 0.079731196 -MSE: 5.213916995899956 train_accuray: 0.9819277\n",
      "epoch: 612 - cost: 0.07848595 -MSE: 5.280845762987614 train_accuray: 0.9819277\n",
      "epoch: 613 - cost: 0.0773277 -MSE: 5.345522605052327 train_accuray: 0.9819277\n",
      "epoch: 614 - cost: 0.07624753 -MSE: 5.4073079227956065 train_accuray: 0.9819277\n",
      "epoch: 615 - cost: 0.075237416 -MSE: 5.466987420221675 train_accuray: 0.9819277\n",
      "epoch: 616 - cost: 0.07429021 -MSE: 5.52421173458092 train_accuray: 0.9879518\n",
      "epoch: 617 - cost: 0.073399514 -MSE: 5.579515925356152 train_accuray: 0.9879518\n",
      "epoch: 618 - cost: 0.072559625 -MSE: 5.632720522097277 train_accuray: 0.9879518\n",
      "epoch: 619 - cost: 0.07176537 -MSE: 5.684199775886624 train_accuray: 0.9879518\n",
      "epoch: 620 - cost: 0.07101228 -MSE: 5.733865207366089 train_accuray: 0.9879518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 621 - cost: 0.0702962 -MSE: 5.781993182140916 train_accuray: 0.9879518\n",
      "epoch: 622 - cost: 0.06961369 -MSE: 5.828543398341382 train_accuray: 0.9879518\n",
      "epoch: 623 - cost: 0.068961434 -MSE: 5.873724678857586 train_accuray: 0.9879518\n",
      "epoch: 624 - cost: 0.068336554 -MSE: 5.917523044915718 train_accuray: 0.9879518\n",
      "epoch: 625 - cost: 0.067736566 -MSE: 5.960102516902073 train_accuray: 0.9879518\n",
      "epoch: 626 - cost: 0.06715921 -MSE: 6.001462872133962 train_accuray: 0.9879518\n",
      "epoch: 627 - cost: 0.06660249 -MSE: 6.041733988207 train_accuray: 0.9879518\n",
      "epoch: 628 - cost: 0.06606455 -MSE: 6.080924529628541 train_accuray: 0.9879518\n",
      "epoch: 629 - cost: 0.065543845 -MSE: 6.119140617234636 train_accuray: 0.9879518\n",
      "epoch: 630 - cost: 0.06503894 -MSE: 6.156392528087712 train_accuray: 0.9879518\n",
      "epoch: 631 - cost: 0.06454851 -MSE: 6.192768314235038 train_accuray: 0.9879518\n",
      "epoch: 632 - cost: 0.064071506 -MSE: 6.228281278570705 train_accuray: 0.9879518\n",
      "epoch: 633 - cost: 0.063606754 -MSE: 6.263008521007718 train_accuray: 0.9879518\n",
      "epoch: 634 - cost: 0.06315348 -MSE: 6.2969576213783665 train_accuray: 0.9879518\n",
      "epoch: 635 - cost: 0.062710784 -MSE: 6.330200442854575 train_accuray: 0.9879518\n",
      "epoch: 636 - cost: 0.062277876 -MSE: 6.362739925857684 train_accuray: 0.9879518\n",
      "epoch: 637 - cost: 0.06185416 -MSE: 6.394638743930397 train_accuray: 0.9879518\n",
      "epoch: 638 - cost: 0.061438955 -MSE: 6.42590391040407 train_accuray: 0.9879518\n",
      "epoch: 639 - cost: 0.061031826 -MSE: 6.456591142626086 train_accuray: 0.9879518\n",
      "epoch: 640 - cost: 0.060632136 -MSE: 6.486700934089088 train_accuray: 0.9879518\n",
      "epoch: 641 - cost: 0.060239498 -MSE: 6.5162847587322075 train_accuray: 0.9879518\n",
      "epoch: 642 - cost: 0.059853513 -MSE: 6.545345350452739 train_accuray: 0.9879518\n",
      "epoch: 643 - cost: 0.059473768 -MSE: 6.57392964889825 train_accuray: 0.9879518\n",
      "epoch: 644 - cost: 0.05909996 -MSE: 6.602033897085869 train_accuray: 0.9879518\n",
      "epoch: 645 - cost: 0.058731686 -MSE: 6.629704350221909 train_accuray: 0.9879518\n",
      "epoch: 646 - cost: 0.058368757 -MSE: 6.656938269367177 train_accuray: 0.9879518\n",
      "epoch: 647 - cost: 0.058010895 -MSE: 6.68377515946978 train_accuray: 0.9879518\n",
      "epoch: 648 - cost: 0.057657864 -MSE: 6.710214329384953 train_accuray: 0.9879518\n",
      "epoch: 649 - cost: 0.057309363 -MSE: 6.736291312157285 train_accuray: 0.9879518\n",
      "epoch: 650 - cost: 0.0569653 -MSE: 6.761999321464711 train_accuray: 0.9879518\n",
      "epoch: 651 - cost: 0.05662542 -MSE: 6.787378498014543 train_accuray: 0.9879518\n",
      "epoch: 652 - cost: 0.056289587 -MSE: 6.812417750489738 train_accuray: 0.9879518\n",
      "epoch: 653 - cost: 0.055957615 -MSE: 6.837155751710332 train_accuray: 0.9879518\n",
      "epoch: 654 - cost: 0.05562936 -MSE: 6.861582265425754 train_accuray: 0.9879518\n",
      "epoch: 655 - cost: 0.05530472 -MSE: 6.885732179394123 train_accuray: 0.9879518\n",
      "epoch: 656 - cost: 0.054983534 -MSE: 6.909592564874082 train_accuray: 0.9879518\n",
      "epoch: 657 - cost: 0.054665715 -MSE: 6.933200937202096 train_accuray: 0.9879518\n",
      "epoch: 658 - cost: 0.05435114 -MSE: 6.95653688179409 train_accuray: 0.9879518\n",
      "epoch: 659 - cost: 0.05403969 -MSE: 6.979647767091773 train_accuray: 0.9879518\n",
      "epoch: 660 - cost: 0.053731315 -MSE: 7.00250135486055 train_accuray: 0.9879518\n",
      "epoch: 661 - cost: 0.053425897 -MSE: 7.0251515600671235 train_accuray: 0.9879518\n",
      "epoch: 662 - cost: 0.053123377 -MSE: 7.04755995330318 train_accuray: 0.9879518\n",
      "epoch: 663 - cost: 0.052823674 -MSE: 7.069783086329649 train_accuray: 0.9879518\n",
      "epoch: 664 - cost: 0.05252673 -MSE: 7.091777494137946 train_accuray: 0.9879518\n",
      "epoch: 665 - cost: 0.052232448 -MSE: 7.113606254595946 train_accuray: 0.9879518\n",
      "epoch: 666 - cost: 0.051940788 -MSE: 7.1352156965055125 train_accuray: 0.9879518\n",
      "epoch: 667 - cost: 0.05165171 -MSE: 7.156678849072253 train_accuray: 0.9879518\n",
      "epoch: 668 - cost: 0.051365133 -MSE: 7.177932035689064 train_accuray: 0.9879518\n",
      "epoch: 669 - cost: 0.05108103 -MSE: 7.199054390139655 train_accuray: 0.9879518\n",
      "epoch: 670 - cost: 0.05079933 -MSE: 7.219972902749173 train_accuray: 0.9879518\n",
      "epoch: 671 - cost: 0.050519973 -MSE: 7.240785319877362 train_accuray: 0.9879518\n",
      "epoch: 672 - cost: 0.050243028 -MSE: 7.261388588495506 train_accuray: 0.9879518\n",
      "epoch: 673 - cost: 0.049968295 -MSE: 7.2819097758370965 train_accuray: 0.9879518\n",
      "epoch: 674 - cost: 0.049695883 -MSE: 7.3022184413264215 train_accuray: 0.9879518\n",
      "epoch: 675 - cost: 0.049425635 -MSE: 7.322470405970721 train_accuray: 0.9879518\n",
      "epoch: 676 - cost: 0.049157612 -MSE: 7.3425023314174735 train_accuray: 0.9879518\n",
      "epoch: 677 - cost: 0.048891738 -MSE: 7.362506489407557 train_accuray: 0.9879518\n",
      "epoch: 678 - cost: 0.048628002 -MSE: 7.382274906550789 train_accuray: 0.9879518\n",
      "epoch: 679 - cost: 0.048366334 -MSE: 7.402048869786501 train_accuray: 0.9879518\n",
      "epoch: 680 - cost: 0.0481068 -MSE: 7.421568238780744 train_accuray: 0.9879518\n",
      "epoch: 681 - cost: 0.04784927 -MSE: 7.44113244000114 train_accuray: 0.9879518\n",
      "epoch: 682 - cost: 0.047593765 -MSE: 7.460408701145589 train_accuray: 0.9879518\n",
      "epoch: 683 - cost: 0.047340296 -MSE: 7.479783011201242 train_accuray: 0.9879518\n",
      "epoch: 684 - cost: 0.047088765 -MSE: 7.498825587085632 train_accuray: 0.9879518\n",
      "epoch: 685 - cost: 0.046839226 -MSE: 7.5180285621630345 train_accuray: 0.9879518\n",
      "epoch: 686 - cost: 0.046591602 -MSE: 7.536840502287297 train_accuray: 0.9879518\n",
      "epoch: 687 - cost: 0.046345863 -MSE: 7.555893491293127 train_accuray: 0.9879518\n",
      "epoch: 688 - cost: 0.04610209 -MSE: 7.574472918080033 train_accuray: 0.9879518\n",
      "epoch: 689 - cost: 0.045860153 -MSE: 7.59340115020742 train_accuray: 0.9879518\n",
      "epoch: 690 - cost: 0.0456201 -MSE: 7.611745425217097 train_accuray: 0.9879518\n",
      "epoch: 691 - cost: 0.045381878 -MSE: 7.630571151133433 train_accuray: 0.9879518\n",
      "epoch: 692 - cost: 0.045145493 -MSE: 7.648671953092405 train_accuray: 0.9879518\n",
      "epoch: 693 - cost: 0.0449109 -MSE: 7.667426211256275 train_accuray: 0.9879518\n",
      "epoch: 694 - cost: 0.04467811 -MSE: 7.685267689568667 train_accuray: 0.9879518\n",
      "epoch: 695 - cost: 0.0444471 -MSE: 7.70398784959931 train_accuray: 0.9879518\n",
      "epoch: 696 - cost: 0.044217847 -MSE: 7.7215419688095706 train_accuray: 0.9879518\n",
      "epoch: 697 - cost: 0.043990348 -MSE: 7.740274902224642 train_accuray: 0.9879518\n",
      "epoch: 698 - cost: 0.04376458 -MSE: 7.75750611678618 train_accuray: 0.9879518\n",
      "epoch: 699 - cost: 0.043540526 -MSE: 7.776305960764965 train_accuray: 0.9879518\n",
      "epoch: 700 - cost: 0.04331816 -MSE: 7.793164421037196 train_accuray: 0.9879518\n",
      "epoch: 701 - cost: 0.04309751 -MSE: 7.812104629875158 train_accuray: 0.9879518\n",
      "epoch: 702 - cost: 0.04287854 -MSE: 7.828517893617648 train_accuray: 0.9879518\n",
      "epoch: 703 - cost: 0.042661224 -MSE: 7.8476922728116385 train_accuray: 0.9939759\n",
      "epoch: 704 - cost: 0.042445548 -MSE: 7.863566726754202 train_accuray: 0.9939759\n",
      "epoch: 705 - cost: 0.04223155 -MSE: 7.883092554167304 train_accuray: 0.9939759\n",
      "epoch: 706 - cost: 0.04201912 -MSE: 7.898303016265135 train_accuray: 0.9939759\n",
      "epoch: 707 - cost: 0.04180839 -MSE: 7.918337755115637 train_accuray: 0.9939759\n",
      "epoch: 708 - cost: 0.04159922 -MSE: 7.932713607401906 train_accuray: 0.9939759\n",
      "epoch: 709 - cost: 0.04139168 -MSE: 7.953459238385552 train_accuray: 0.9939759\n",
      "epoch: 710 - cost: 0.04118573 -MSE: 7.966780421556445 train_accuray: 0.9939759\n",
      "epoch: 711 - cost: 0.040981386 -MSE: 7.988497565015845 train_accuray: 0.9939759\n",
      "epoch: 712 - cost: 0.040778648 -MSE: 8.00047488901667 train_accuray: 0.9939759\n",
      "epoch: 713 - cost: 0.040577535 -MSE: 8.023502637883878 train_accuray: 0.9939759\n",
      "epoch: 714 - cost: 0.040378045 -MSE: 8.03375721652404 train_accuray: 0.9939759\n",
      "epoch: 715 - cost: 0.040180195 -MSE: 8.058532586669068 train_accuray: 1.0\n",
      "epoch: 716 - cost: 0.03998405 -MSE: 8.066573600638005 train_accuray: 1.0\n",
      "epoch: 717 - cost: 0.039789606 -MSE: 8.093662926266648 train_accuray: 1.0\n",
      "epoch: 718 - cost: 0.039596997 -MSE: 8.098854849697922 train_accuray: 1.0\n",
      "epoch: 719 - cost: 0.039406225 -MSE: 8.128987383087072 train_accuray: 1.0\n",
      "epoch: 720 - cost: 0.0392175 -MSE: 8.130511600393818 train_accuray: 1.0\n",
      "epoch: 721 - cost: 0.039030865 -MSE: 8.164620485941342 train_accuray: 1.0\n",
      "epoch: 722 - cost: 0.03884665 -MSE: 8.161430992160357 train_accuray: 1.0\n",
      "epoch: 723 - cost: 0.038664874 -MSE: 8.200706165833017 train_accuray: 1.0\n",
      "epoch: 724 - cost: 0.038486246 -MSE: 8.191467360735798 train_accuray: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 725 - cost: 0.0383106 -MSE: 8.237429553542356 train_accuray: 1.0\n",
      "epoch: 726 - cost: 0.03813934 -MSE: 8.220437073043456 train_accuray: 1.0\n",
      "epoch: 727 - cost: 0.037971854 -MSE: 8.275010161289853 train_accuray: 1.0\n",
      "epoch: 728 - cost: 0.037810896 -MSE: 8.248113905212987 train_accuray: 1.0\n",
      "epoch: 729 - cost: 0.037654836 -MSE: 8.313742303857754 train_accuray: 1.0\n",
      "epoch: 730 - cost: 0.037509095 -MSE: 8.274213106955203 train_accuray: 1.0\n",
      "epoch: 731 - cost: 0.037369616 -MSE: 8.353991272437428 train_accuray: 1.0\n",
      "epoch: 732 - cost: 0.037247203 -MSE: 8.298373078485833 train_accuray: 1.0\n",
      "epoch: 733 - cost: 0.03713261 -MSE: 8.396250330461118 train_accuray: 1.0\n",
      "epoch: 734 - cost: 0.037047356 -MSE: 8.320124582358007 train_accuray: 1.0\n",
      "epoch: 735 - cost: 0.0369711 -MSE: 8.441196983859161 train_accuray: 1.0\n",
      "epoch: 736 - cost: 0.036947347 -MSE: 8.33883207723896 train_accuray: 1.0\n",
      "epoch: 737 - cost: 0.036932245 -MSE: 8.48984030747742 train_accuray: 1.0\n",
      "epoch: 738 - cost: 0.03701499 -MSE: 8.353600090490174 train_accuray: 1.0\n",
      "epoch: 739 - cost: 0.037102733 -MSE: 8.543827187452312 train_accuray: 1.0\n",
      "epoch: 740 - cost: 0.037383325 -MSE: 8.363062740689331 train_accuray: 0.9939759\n",
      "epoch: 741 - cost: 0.037660085 -MSE: 8.606226672543563 train_accuray: 1.0\n",
      "epoch: 742 - cost: 0.038349 -MSE: 8.36499380842374 train_accuray: 0.9939759\n",
      "epoch: 743 - cost: 0.039029825 -MSE: 8.683731333161063 train_accuray: 1.0\n",
      "epoch: 744 - cost: 0.0407196 -MSE: 8.355661127095072 train_accuray: 0.9939759\n",
      "epoch: 745 - cost: 0.04251315 -MSE: 8.794550376641157 train_accuray: 1.0\n",
      "epoch: 746 - cost: 0.04749659 -MSE: 8.330578176565304 train_accuray: 0.9879518\n",
      "epoch: 747 - cost: 0.054005843 -MSE: 9.007363120422484 train_accuray: 0.9939759\n",
      "epoch: 748 - cost: 0.07749596 -MSE: 8.321024405283211 train_accuray: 0.96385545\n",
      "epoch: 749 - cost: 0.12521629 -MSE: 9.755575459976521 train_accuray: 0.9518072\n",
      "epoch: 750 - cost: 0.43418053 -MSE: 9.30968416248344 train_accuray: 0.82530123\n",
      "epoch: 751 - cost: 1.9639753 -MSE: 19.259675990448624 train_accuray: 0.59638554\n",
      "epoch: 752 - cost: 4.3425894 -MSE: 28.02505676035012 train_accuray: 0.560241\n",
      "epoch: 753 - cost: 0.827062 -MSE: 3.647483555550703 train_accuray: 0.57831323\n",
      "epoch: 754 - cost: 0.95493186 -MSE: 5.046917863185502 train_accuray: 0.46385542\n",
      "epoch: 755 - cost: 1.4100288 -MSE: 5.762786377059606 train_accuray: 0.560241\n",
      "epoch: 756 - cost: 0.92947423 -MSE: 4.802175061962442 train_accuray: 0.43975905\n",
      "epoch: 757 - cost: 1.2043316 -MSE: 5.161741688181379 train_accuray: 0.560241\n",
      "epoch: 758 - cost: 0.7765309 -MSE: 4.198357640530776 train_accuray: 0.4698795\n",
      "epoch: 759 - cost: 0.9053348 -MSE: 4.328724690427976 train_accuray: 0.60240966\n",
      "epoch: 760 - cost: 0.7184366 -MSE: 3.9437023643504188 train_accuray: 0.5\n",
      "epoch: 761 - cost: 0.82807225 -MSE: 4.257765445328141 train_accuray: 0.6204819\n",
      "epoch: 762 - cost: 0.64226246 -MSE: 3.689763679528812 train_accuray: 0.5481928\n",
      "epoch: 763 - cost: 0.71071184 -MSE: 4.088539065768062 train_accuray: 0.67469877\n",
      "epoch: 764 - cost: 0.5579298 -MSE: 3.4280630984105174 train_accuray: 0.6626506\n",
      "epoch: 765 - cost: 0.6017165 -MSE: 3.9347884849775636 train_accuray: 0.72289157\n",
      "epoch: 766 - cost: 0.48467875 -MSE: 3.213803639058532 train_accuray: 0.7108434\n",
      "epoch: 767 - cost: 0.51311624 -MSE: 3.8199116660468246 train_accuray: 0.7590361\n",
      "epoch: 768 - cost: 0.42658225 -MSE: 3.0548030077205617 train_accuray: 0.8072289\n",
      "epoch: 769 - cost: 0.44574824 -MSE: 3.7393232799716443 train_accuray: 0.7710843\n",
      "epoch: 770 - cost: 0.38190097 -MSE: 2.942566596974022 train_accuray: 0.8614458\n",
      "epoch: 771 - cost: 0.3956758 -MSE: 3.685682006052001 train_accuray: 0.8012048\n",
      "epoch: 772 - cost: 0.34779578 -MSE: 2.8677854628552204 train_accuray: 0.87349397\n",
      "epoch: 773 - cost: 0.35846874 -MSE: 3.6585351751308517 train_accuray: 0.8072289\n",
      "epoch: 774 - cost: 0.32178816 -MSE: 2.824045110586376 train_accuray: 0.88554215\n",
      "epoch: 775 - cost: 0.33055884 -MSE: 3.6593672091960214 train_accuray: 0.8313253\n",
      "epoch: 776 - cost: 0.30165577 -MSE: 2.8069465055352367 train_accuray: 0.8915663\n",
      "epoch: 777 - cost: 0.30891207 -MSE: 3.6855588648413864 train_accuray: 0.8433735\n",
      "epoch: 778 - cost: 0.28518924 -MSE: 2.813216257450326 train_accuray: 0.91566265\n",
      "epoch: 779 - cost: 0.29061133 -MSE: 3.728731970841576 train_accuray: 0.8493976\n",
      "epoch: 780 - cost: 0.26998547 -MSE: 2.839997038693455 train_accuray: 0.92771083\n",
      "epoch: 781 - cost: 0.27257535 -MSE: 3.774940996458678 train_accuray: 0.87349397\n",
      "epoch: 782 - cost: 0.25329763 -MSE: 2.8842783084992965 train_accuray: 0.92771083\n",
      "epoch: 783 - cost: 0.25157407 -MSE: 3.806018424237312 train_accuray: 0.88554215\n",
      "epoch: 784 - cost: 0.2323466 -MSE: 2.9436926935423404 train_accuray: 0.93373495\n",
      "epoch: 785 - cost: 0.22511625 -MSE: 3.805229678734223 train_accuray: 0.89759034\n",
      "epoch: 786 - cost: 0.20571455 -MSE: 3.020341724616376 train_accuray: 0.93373495\n",
      "epoch: 787 - cost: 0.19355719 -MSE: 3.7708075136907944 train_accuray: 0.92168677\n",
      "epoch: 788 - cost: 0.17549296 -MSE: 3.1247006693370256 train_accuray: 0.939759\n",
      "epoch: 789 - cost: 0.16164877 -MSE: 3.7296380203689106 train_accuray: 0.94578314\n",
      "epoch: 790 - cost: 0.14725003 -MSE: 3.2684597530985493 train_accuray: 0.9578313\n",
      "epoch: 791 - cost: 0.1356513 -MSE: 3.7241057639720765 train_accuray: 0.9518072\n",
      "epoch: 792 - cost: 0.1257969 -MSE: 3.4485747212647917 train_accuray: 0.9698795\n",
      "epoch: 793 - cost: 0.117902495 -MSE: 3.7763290392129156 train_accuray: 0.9578313\n",
      "epoch: 794 - cost: 0.11162794 -MSE: 3.647580356603861 train_accuray: 0.96385545\n",
      "epoch: 795 - cost: 0.10654519 -MSE: 3.8806083898080184 train_accuray: 0.97590363\n",
      "epoch: 796 - cost: 0.10240851 -MSE: 3.8486852079197473 train_accuray: 0.9819277\n",
      "epoch: 797 - cost: 0.0989115 -MSE: 4.019642084736868 train_accuray: 0.9819277\n",
      "epoch: 798 - cost: 0.0959142 -MSE: 4.04380633983955 train_accuray: 0.9819277\n",
      "epoch: 799 - cost: 0.093264244 -MSE: 4.177239157136772 train_accuray: 0.9819277\n",
      "epoch: 800 - cost: 0.09089682 -MSE: 4.231011479399366 train_accuray: 0.9819277\n",
      "epoch: 801 - cost: 0.08874036 -MSE: 4.3423325891494615 train_accuray: 0.9819277\n",
      "epoch: 802 - cost: 0.08676446 -MSE: 4.410400553763334 train_accuray: 0.9819277\n",
      "epoch: 803 - cost: 0.084933236 -MSE: 4.508303368525571 train_accuray: 0.9879518\n",
      "epoch: 804 - cost: 0.083229795 -MSE: 4.582299570757944 train_accuray: 0.9819277\n",
      "epoch: 805 - cost: 0.081633955 -MSE: 4.671441208518713 train_accuray: 0.9879518\n",
      "epoch: 806 - cost: 0.08013461 -MSE: 4.7469362169768825 train_accuray: 0.9879518\n",
      "epoch: 807 - cost: 0.078719124 -MSE: 4.8298095193259 train_accuray: 0.9879518\n",
      "epoch: 808 - cost: 0.07737932 -MSE: 4.904523916157505 train_accuray: 0.9879518\n",
      "epoch: 809 - cost: 0.07610666 -MSE: 4.982513472319498 train_accuray: 0.9879518\n",
      "epoch: 810 - cost: 0.07489502 -MSE: 5.055338165088606 train_accuray: 0.9879518\n",
      "epoch: 811 - cost: 0.07373809 -MSE: 5.129271938517177 train_accuray: 0.9879518\n",
      "epoch: 812 - cost: 0.0726313 -MSE: 5.199713395460593 train_accuray: 0.9879518\n",
      "epoch: 813 - cost: 0.071570046 -MSE: 5.270142239939672 train_accuray: 0.9879518\n",
      "epoch: 814 - cost: 0.07055063 -MSE: 5.3380371167763085 train_accuray: 0.9879518\n",
      "epoch: 815 - cost: 0.06956968 -MSE: 5.405348173020129 train_accuray: 0.9879518\n",
      "epoch: 816 - cost: 0.06862438 -MSE: 5.470699116746993 train_accuray: 0.9879518\n",
      "epoch: 817 - cost: 0.067712024 -MSE: 5.535194389665613 train_accuray: 0.9879518\n",
      "epoch: 818 - cost: 0.06683045 -MSE: 5.598081637941541 train_accuray: 0.9879518\n",
      "epoch: 819 - cost: 0.06597763 -MSE: 5.6599947985544565 train_accuray: 0.9879518\n",
      "epoch: 820 - cost: 0.065151736 -MSE: 5.720530369374183 train_accuray: 0.9879518\n",
      "epoch: 821 - cost: 0.06435122 -MSE: 5.780055459348042 train_accuray: 0.9879518\n",
      "epoch: 822 - cost: 0.063574605 -MSE: 5.838357975468563 train_accuray: 0.9879518\n",
      "epoch: 823 - cost: 0.062820606 -MSE: 5.895655934430529 train_accuray: 0.9879518\n",
      "epoch: 824 - cost: 0.06208812 -MSE: 5.951840621740537 train_accuray: 0.9879518\n",
      "epoch: 825 - cost: 0.06137604 -MSE: 6.007047777280496 train_accuray: 0.9879518\n",
      "epoch: 826 - cost: 0.060683396 -MSE: 6.0612231373085095 train_accuray: 0.9879518\n",
      "epoch: 827 - cost: 0.060009312 -MSE: 6.11445167637147 train_accuray: 0.9879518\n",
      "epoch: 828 - cost: 0.059352983 -MSE: 6.166715894740809 train_accuray: 0.9879518\n",
      "epoch: 829 - cost: 0.05871362 -MSE: 6.218070644938169 train_accuray: 0.9879518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 830 - cost: 0.058090582 -MSE: 6.268513500894332 train_accuray: 0.9879518\n",
      "epoch: 831 - cost: 0.057483125 -MSE: 6.318084977511863 train_accuray: 0.9879518\n",
      "epoch: 832 - cost: 0.056890655 -MSE: 6.366789955110177 train_accuray: 0.9879518\n",
      "epoch: 833 - cost: 0.056312613 -MSE: 6.41466013156715 train_accuray: 0.9879518\n",
      "epoch: 834 - cost: 0.055748492 -MSE: 6.461706931078745 train_accuray: 0.9879518\n",
      "epoch: 835 - cost: 0.055197634 -MSE: 6.507954492134904 train_accuray: 0.9879518\n",
      "epoch: 836 - cost: 0.05465966 -MSE: 6.553418750818902 train_accuray: 0.9879518\n",
      "epoch: 837 - cost: 0.054134 -MSE: 6.5981191246696005 train_accuray: 0.9879518\n",
      "epoch: 838 - cost: 0.053620286 -MSE: 6.642070193860971 train_accuray: 0.9879518\n",
      "epoch: 839 - cost: 0.05311806 -MSE: 6.685294903955251 train_accuray: 0.9879518\n",
      "epoch: 840 - cost: 0.052626856 -MSE: 6.727808236424393 train_accuray: 0.9879518\n",
      "epoch: 841 - cost: 0.052146327 -MSE: 6.76962733772577 train_accuray: 0.9879518\n",
      "epoch: 842 - cost: 0.051676065 -MSE: 6.8107701294793515 train_accuray: 0.9879518\n",
      "epoch: 843 - cost: 0.05121567 -MSE: 6.851252950259624 train_accuray: 0.9879518\n",
      "epoch: 844 - cost: 0.050764844 -MSE: 6.891093055083668 train_accuray: 0.9879518\n",
      "epoch: 845 - cost: 0.050323203 -MSE: 6.930306138491303 train_accuray: 0.9879518\n",
      "epoch: 846 - cost: 0.049890455 -MSE: 6.968909844092852 train_accuray: 0.9879518\n",
      "epoch: 847 - cost: 0.04946624 -MSE: 7.006922172283204 train_accuray: 0.9879518\n",
      "epoch: 848 - cost: 0.04905032 -MSE: 7.044356092414814 train_accuray: 0.9939759\n",
      "epoch: 849 - cost: 0.048642308 -MSE: 7.081229473310845 train_accuray: 0.9939759\n",
      "epoch: 850 - cost: 0.04824203 -MSE: 7.11755649370062 train_accuray: 0.9939759\n",
      "epoch: 851 - cost: 0.04784917 -MSE: 7.153354453780058 train_accuray: 0.9939759\n",
      "epoch: 852 - cost: 0.047463432 -MSE: 7.188637482312968 train_accuray: 0.9939759\n",
      "epoch: 853 - cost: 0.04708463 -MSE: 7.223418275582291 train_accuray: 0.9939759\n",
      "epoch: 854 - cost: 0.04671253 -MSE: 7.257716341834716 train_accuray: 0.9939759\n",
      "epoch: 855 - cost: 0.046346877 -MSE: 7.29154155533523 train_accuray: 0.9939759\n",
      "epoch: 856 - cost: 0.045987412 -MSE: 7.324909587229531 train_accuray: 0.9939759\n",
      "epoch: 857 - cost: 0.045634024 -MSE: 7.357834939386961 train_accuray: 0.9939759\n",
      "epoch: 858 - cost: 0.04528645 -MSE: 7.390331597111638 train_accuray: 0.9939759\n",
      "epoch: 859 - cost: 0.044944532 -MSE: 7.422410329204005 train_accuray: 0.9939759\n",
      "epoch: 860 - cost: 0.044608098 -MSE: 7.454086796195053 train_accuray: 0.9939759\n",
      "epoch: 861 - cost: 0.044276897 -MSE: 7.485369763587631 train_accuray: 0.9939759\n",
      "epoch: 862 - cost: 0.043950856 -MSE: 7.516274829263626 train_accuray: 0.9939759\n",
      "epoch: 863 - cost: 0.04362977 -MSE: 7.546813156533277 train_accuray: 0.9939759\n",
      "epoch: 864 - cost: 0.043313496 -MSE: 7.576994588771292 train_accuray: 0.9939759\n",
      "epoch: 865 - cost: 0.043001935 -MSE: 7.606831557751434 train_accuray: 0.9939759\n",
      "epoch: 866 - cost: 0.042694863 -MSE: 7.636336784060583 train_accuray: 0.9939759\n",
      "epoch: 867 - cost: 0.042392224 -MSE: 7.665515406182346 train_accuray: 0.9939759\n",
      "epoch: 868 - cost: 0.042093884 -MSE: 7.694382918318984 train_accuray: 0.9939759\n",
      "epoch: 869 - cost: 0.041799672 -MSE: 7.722945640721153 train_accuray: 0.9939759\n",
      "epoch: 870 - cost: 0.04150951 -MSE: 7.751217383474121 train_accuray: 0.9939759\n",
      "epoch: 871 - cost: 0.041223295 -MSE: 7.779202196773778 train_accuray: 0.9939759\n",
      "epoch: 872 - cost: 0.040940907 -MSE: 7.806913861362178 train_accuray: 0.9939759\n",
      "epoch: 873 - cost: 0.040662255 -MSE: 7.834358239130073 train_accuray: 0.9939759\n",
      "epoch: 874 - cost: 0.040387254 -MSE: 7.861543990118903 train_accuray: 0.9939759\n",
      "epoch: 875 - cost: 0.040115777 -MSE: 7.8884789562671465 train_accuray: 0.9939759\n",
      "epoch: 876 - cost: 0.039847776 -MSE: 7.915171873677788 train_accuray: 0.9939759\n",
      "epoch: 877 - cost: 0.039583147 -MSE: 7.941629408722874 train_accuray: 0.9939759\n",
      "epoch: 878 - cost: 0.03932183 -MSE: 7.96786003522683 train_accuray: 0.9939759\n",
      "epoch: 879 - cost: 0.039063714 -MSE: 7.993868833310633 train_accuray: 0.9939759\n",
      "epoch: 880 - cost: 0.038808767 -MSE: 8.019663161078833 train_accuray: 0.9939759\n",
      "epoch: 881 - cost: 0.038556907 -MSE: 8.045250868675602 train_accuray: 0.9939759\n",
      "epoch: 882 - cost: 0.038308024 -MSE: 8.07063714294338 train_accuray: 0.9939759\n",
      "epoch: 883 - cost: 0.038062114 -MSE: 8.095828086501047 train_accuray: 0.9939759\n",
      "epoch: 884 - cost: 0.03781906 -MSE: 8.12082979406926 train_accuray: 0.9939759\n",
      "epoch: 885 - cost: 0.037578866 -MSE: 8.145644746883344 train_accuray: 0.9939759\n",
      "epoch: 886 - cost: 0.037341442 -MSE: 8.17028292772629 train_accuray: 0.9939759\n",
      "epoch: 887 - cost: 0.037106708 -MSE: 8.19474663923295 train_accuray: 0.9939759\n",
      "epoch: 888 - cost: 0.036874633 -MSE: 8.219042102587215 train_accuray: 0.9939759\n",
      "epoch: 889 - cost: 0.03664518 -MSE: 8.243173865819703 train_accuray: 0.9939759\n",
      "epoch: 890 - cost: 0.0364183 -MSE: 8.267144742331581 train_accuray: 0.9939759\n",
      "epoch: 891 - cost: 0.036193907 -MSE: 8.290961424536368 train_accuray: 0.9939759\n",
      "epoch: 892 - cost: 0.035971988 -MSE: 8.314627333948234 train_accuray: 0.9939759\n",
      "epoch: 893 - cost: 0.035752486 -MSE: 8.338143821997969 train_accuray: 0.9939759\n",
      "epoch: 894 - cost: 0.03553535 -MSE: 8.361519864607224 train_accuray: 0.9939759\n",
      "epoch: 895 - cost: 0.035320584 -MSE: 8.384753870581969 train_accuray: 0.9939759\n",
      "epoch: 896 - cost: 0.0351081 -MSE: 8.407852637371455 train_accuray: 0.9939759\n",
      "epoch: 897 - cost: 0.034897875 -MSE: 8.430820514615759 train_accuray: 0.9939759\n",
      "epoch: 898 - cost: 0.03468986 -MSE: 8.453655721147914 train_accuray: 0.9939759\n",
      "epoch: 899 - cost: 0.03448404 -MSE: 8.476366829118932 train_accuray: 0.9939759\n",
      "epoch: 900 - cost: 0.034280345 -MSE: 8.498953891789752 train_accuray: 0.9939759\n",
      "epoch: 901 - cost: 0.03407877 -MSE: 8.521420958592852 train_accuray: 0.9939759\n",
      "epoch: 902 - cost: 0.033879284 -MSE: 8.543769945641287 train_accuray: 0.9939759\n",
      "epoch: 903 - cost: 0.03368186 -MSE: 8.566003841832014 train_accuray: 0.9939759\n",
      "epoch: 904 - cost: 0.033486437 -MSE: 8.588125043999664 train_accuray: 0.9939759\n",
      "epoch: 905 - cost: 0.033292964 -MSE: 8.610136805164144 train_accuray: 0.9939759\n",
      "epoch: 906 - cost: 0.033101488 -MSE: 8.632039178070348 train_accuray: 0.9939759\n",
      "epoch: 907 - cost: 0.032911915 -MSE: 8.653836181822397 train_accuray: 0.9939759\n",
      "epoch: 908 - cost: 0.032724224 -MSE: 8.675529928787382 train_accuray: 0.9939759\n",
      "epoch: 909 - cost: 0.032538403 -MSE: 8.697122126234891 train_accuray: 0.9939759\n",
      "epoch: 910 - cost: 0.032354433 -MSE: 8.718614660263865 train_accuray: 0.9939759\n",
      "epoch: 911 - cost: 0.032172248 -MSE: 8.740010523614691 train_accuray: 0.9939759\n",
      "epoch: 912 - cost: 0.031991865 -MSE: 8.761310569180301 train_accuray: 0.9939759\n",
      "epoch: 913 - cost: 0.031813253 -MSE: 8.78251474692079 train_accuray: 0.9939759\n",
      "epoch: 914 - cost: 0.03163635 -MSE: 8.803628757966257 train_accuray: 0.9939759\n",
      "epoch: 915 - cost: 0.03146117 -MSE: 8.82465202732857 train_accuray: 0.9939759\n",
      "epoch: 916 - cost: 0.031287663 -MSE: 8.845584698744243 train_accuray: 0.9939759\n",
      "epoch: 917 - cost: 0.031115822 -MSE: 8.866430671244716 train_accuray: 0.9939759\n",
      "epoch: 918 - cost: 0.030945629 -MSE: 8.887188801587135 train_accuray: 0.9939759\n",
      "epoch: 919 - cost: 0.030777067 -MSE: 8.907860520276737 train_accuray: 0.9939759\n",
      "epoch: 920 - cost: 0.030610036 -MSE: 8.92845001432549 train_accuray: 0.9939759\n",
      "epoch: 921 - cost: 0.030444626 -MSE: 8.948958064815619 train_accuray: 0.9939759\n",
      "epoch: 922 - cost: 0.030280754 -MSE: 8.969384415966251 train_accuray: 1.0\n",
      "epoch: 923 - cost: 0.030118424 -MSE: 8.98973086015517 train_accuray: 1.0\n",
      "epoch: 924 - cost: 0.029957568 -MSE: 9.009996909206482 train_accuray: 1.0\n",
      "epoch: 925 - cost: 0.029798204 -MSE: 9.030186287793233 train_accuray: 1.0\n",
      "epoch: 926 - cost: 0.029640326 -MSE: 9.050298854560756 train_accuray: 1.0\n",
      "epoch: 927 - cost: 0.029483879 -MSE: 9.070337952945234 train_accuray: 1.0\n",
      "epoch: 928 - cost: 0.029328883 -MSE: 9.090298769521464 train_accuray: 1.0\n",
      "epoch: 929 - cost: 0.029175278 -MSE: 9.110185708599166 train_accuray: 1.0\n",
      "epoch: 930 - cost: 0.029023075 -MSE: 9.130003455191156 train_accuray: 1.0\n",
      "epoch: 931 - cost: 0.028872263 -MSE: 9.149746461446016 train_accuray: 1.0\n",
      "epoch: 932 - cost: 0.028722763 -MSE: 9.169419961451563 train_accuray: 1.0\n",
      "epoch: 933 - cost: 0.028574614 -MSE: 9.189020908038835 train_accuray: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 934 - cost: 0.028427793 -MSE: 9.208555234086774 train_accuray: 1.0\n",
      "epoch: 935 - cost: 0.028282275 -MSE: 9.228018100616843 train_accuray: 1.0\n",
      "epoch: 936 - cost: 0.028138055 -MSE: 9.247414243722991 train_accuray: 1.0\n",
      "epoch: 937 - cost: 0.027995074 -MSE: 9.266743481242184 train_accuray: 1.0\n",
      "epoch: 938 - cost: 0.02785336 -MSE: 9.286006936872354 train_accuray: 1.0\n",
      "epoch: 939 - cost: 0.02771289 -MSE: 9.305202715954453 train_accuray: 1.0\n",
      "epoch: 940 - cost: 0.027573632 -MSE: 9.324336928698537 train_accuray: 1.0\n",
      "epoch: 941 - cost: 0.027435604 -MSE: 9.343404832180319 train_accuray: 1.0\n",
      "epoch: 942 - cost: 0.027298748 -MSE: 9.362407461744349 train_accuray: 1.0\n",
      "epoch: 943 - cost: 0.027163083 -MSE: 9.381349720353578 train_accuray: 1.0\n",
      "epoch: 944 - cost: 0.027028592 -MSE: 9.400229702159956 train_accuray: 1.0\n",
      "epoch: 945 - cost: 0.02689522 -MSE: 9.419045966916448 train_accuray: 1.0\n",
      "epoch: 946 - cost: 0.026763009 -MSE: 9.437801185922261 train_accuray: 1.0\n",
      "epoch: 947 - cost: 0.026631897 -MSE: 9.456497453662658 train_accuray: 1.0\n",
      "epoch: 948 - cost: 0.026501885 -MSE: 9.475132358293408 train_accuray: 1.0\n",
      "epoch: 949 - cost: 0.026372967 -MSE: 9.493707767998057 train_accuray: 1.0\n",
      "epoch: 950 - cost: 0.026245145 -MSE: 9.512224806220289 train_accuray: 1.0\n",
      "epoch: 951 - cost: 0.026118396 -MSE: 9.530684256352917 train_accuray: 1.0\n",
      "epoch: 952 - cost: 0.025992686 -MSE: 9.549083119046397 train_accuray: 1.0\n",
      "epoch: 953 - cost: 0.02586802 -MSE: 9.567424407853938 train_accuray: 1.0\n",
      "epoch: 954 - cost: 0.025744384 -MSE: 9.585711175361993 train_accuray: 1.0\n",
      "epoch: 955 - cost: 0.025621762 -MSE: 9.603940572894885 train_accuray: 1.0\n",
      "epoch: 956 - cost: 0.025500165 -MSE: 9.622112441120946 train_accuray: 1.0\n",
      "epoch: 957 - cost: 0.02537952 -MSE: 9.640230988404129 train_accuray: 1.0\n",
      "epoch: 958 - cost: 0.025259877 -MSE: 9.658294130259137 train_accuray: 1.0\n",
      "epoch: 959 - cost: 0.025141196 -MSE: 9.676302211569272 train_accuray: 1.0\n",
      "epoch: 960 - cost: 0.025023486 -MSE: 9.694255819790975 train_accuray: 1.0\n",
      "epoch: 961 - cost: 0.02490674 -MSE: 9.712154863996412 train_accuray: 1.0\n",
      "epoch: 962 - cost: 0.024790892 -MSE: 9.730002130850709 train_accuray: 1.0\n",
      "epoch: 963 - cost: 0.024675978 -MSE: 9.74779528616019 train_accuray: 1.0\n",
      "epoch: 964 - cost: 0.024561986 -MSE: 9.765537053063502 train_accuray: 1.0\n",
      "epoch: 965 - cost: 0.024448887 -MSE: 9.7832266254922 train_accuray: 1.0\n",
      "epoch: 966 - cost: 0.024336694 -MSE: 9.80086404071216 train_accuray: 1.0\n",
      "epoch: 967 - cost: 0.024225384 -MSE: 9.818450908198766 train_accuray: 1.0\n",
      "epoch: 968 - cost: 0.024114938 -MSE: 9.835986683453543 train_accuray: 1.0\n",
      "epoch: 969 - cost: 0.024005357 -MSE: 9.85347142241218 train_accuray: 1.0\n",
      "epoch: 970 - cost: 0.023896636 -MSE: 9.870906198443974 train_accuray: 1.0\n",
      "epoch: 971 - cost: 0.023788756 -MSE: 9.888291001814588 train_accuray: 1.0\n",
      "epoch: 972 - cost: 0.023681715 -MSE: 9.905625792435869 train_accuray: 1.0\n",
      "epoch: 973 - cost: 0.023575498 -MSE: 9.922911930037177 train_accuray: 1.0\n",
      "epoch: 974 - cost: 0.023470087 -MSE: 9.940150016837766 train_accuray: 1.0\n",
      "epoch: 975 - cost: 0.023365477 -MSE: 9.957340044913499 train_accuray: 1.0\n",
      "epoch: 976 - cost: 0.023261681 -MSE: 9.974481330497689 train_accuray: 1.0\n",
      "epoch: 977 - cost: 0.023158671 -MSE: 9.99157427963675 train_accuray: 1.0\n",
      "epoch: 978 - cost: 0.023056429 -MSE: 10.008621573898113 train_accuray: 1.0\n",
      "epoch: 979 - cost: 0.02295499 -MSE: 10.025622281982976 train_accuray: 1.0\n",
      "epoch: 980 - cost: 0.022854296 -MSE: 10.042574912754118 train_accuray: 1.0\n",
      "epoch: 981 - cost: 0.022754328 -MSE: 10.059482441577622 train_accuray: 1.0\n",
      "epoch: 982 - cost: 0.022655146 -MSE: 10.076343126149949 train_accuray: 1.0\n",
      "epoch: 983 - cost: 0.022556698 -MSE: 10.093157430745316 train_accuray: 1.0\n",
      "epoch: 984 - cost: 0.022458978 -MSE: 10.109926821068617 train_accuray: 1.0\n",
      "epoch: 985 - cost: 0.022361979 -MSE: 10.12665110546109 train_accuray: 1.0\n",
      "epoch: 986 - cost: 0.022265682 -MSE: 10.143330568474017 train_accuray: 1.0\n",
      "epoch: 987 - cost: 0.022170115 -MSE: 10.159965911851529 train_accuray: 1.0\n",
      "epoch: 988 - cost: 0.02207523 -MSE: 10.176557482232042 train_accuray: 1.0\n",
      "epoch: 989 - cost: 0.02198104 -MSE: 10.193104663175037 train_accuray: 1.0\n",
      "epoch: 990 - cost: 0.021887526 -MSE: 10.20960716560712 train_accuray: 1.0\n",
      "epoch: 991 - cost: 0.021794723 -MSE: 10.226067362028717 train_accuray: 1.0\n",
      "epoch: 992 - cost: 0.021702556 -MSE: 10.24248432415185 train_accuray: 1.0\n",
      "epoch: 993 - cost: 0.02161108 -MSE: 10.258858934538479 train_accuray: 1.0\n",
      "epoch: 994 - cost: 0.021520259 -MSE: 10.275190908994603 train_accuray: 1.0\n",
      "epoch: 995 - cost: 0.021430084 -MSE: 10.29148051974473 train_accuray: 1.0\n",
      "epoch: 996 - cost: 0.021340538 -MSE: 10.30772747316976 train_accuray: 1.0\n",
      "epoch: 997 - cost: 0.021251654 -MSE: 10.323933722567899 train_accuray: 1.0\n",
      "epoch: 998 - cost: 0.02116338 -MSE: 10.340100056592448 train_accuray: 1.0\n",
      "epoch: 999 - cost: 0.021075742 -MSE: 10.356223225792517 train_accuray: 1.0\n"
     ]
    }
   ],
   "source": [
    "#calculating cost and accuracy for each epochs\n",
    "mse_history=[]\n",
    "accuracy_history=[]\n",
    "for epoch in range(training_epochs):\n",
    "    sess.run(training_step, feed_dict={x: xtrain,y_: ytrain})\n",
    "    cost=sess.run(cost_function,feed_dict={x: xtrain,y_: ytrain})\n",
    "    sess.run(cost_function, feed_dict={x: xtrain,y_: ytrain})\n",
    "    cost_history=np.append(cost_history,cost)\n",
    "    correct_prediction=tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    pred_y=sess.run(y,feed_dict={x:xtest})\n",
    "    mse=tf.reduce_mean(tf.square(pred_y-ytest))\n",
    "    mse_=sess.run(mse)\n",
    "    mse_history.append(mse_)\n",
    "    accuracy=(sess.run(accuracy,feed_dict={x:xtrain,y_:ytrain}))\n",
    "    accuracy_history.append(accuracy)\n",
    "    print('epoch:',epoch,'-','cost:',cost, \"-MSE:\",mse_,\"train_accuray:\",accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path=saver.save(sess,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.8333333\n"
     ]
    }
   ],
   "source": [
    "#printing final accuracy\n",
    "correct_prediction=tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "print(\"test accuracy:\",(sess.run(accuracy,feed_dict={x:xtest,y_:ytest})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:10.3562\n"
     ]
    }
   ],
   "source": [
    "#printing final mse\n",
    "pred_y=sess.run(y,feed_dict={x:xtest})\n",
    "mse=tf.reduce_mean(tf.square(pred_y-ytest))\n",
    "print(\"MSE:%.4F\" % sess.run(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting on some given set of data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
